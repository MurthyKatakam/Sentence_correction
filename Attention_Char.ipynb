{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attention_Char.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3u462KOCgVC"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,GRU,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "import six\n",
        "from tensorflow.keras.utils import deserialize_keras_object\n",
        "from joblib import dump, load\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,GRU,Embedding,Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow.keras.layers import concatenate, Lambda\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UylqAaZMgCqT"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbpUygy06BEx",
        "outputId": "f4234a9d-9802-4444-cdb6-b5fa99278080"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLVakVWSHXT"
      },
      "source": [
        "\n",
        "# def preparing_data(file_name):\n",
        "#     files=open(file_name,'r')\n",
        "#     sentences=files.readlines()\n",
        "#     files.close()\n",
        "#     # print(lines)\n",
        "#     sms_text=[]\n",
        "#     english_text=[]\n",
        "\n",
        "#     for index,sentence in enumerate(sentences):\n",
        "#         if index%3==0:\n",
        "#            sms_text.append(sentence.strip())\n",
        "#         elif index%3==1:\n",
        "#             english_text.append(sentence.strip())\n",
        "#         else:\n",
        "#             pass\n",
        "#     return sms_text,english_text             \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZpVAyljSLex"
      },
      "source": [
        "# file_name='/content/drive/MyDrive/Applied Ai Course assignments/Assiginments/CS2/en2cn-2k.en2nen2cn'\n",
        "# sms_text,english_text=preparing_data(file_name)\n",
        "# print(len(sms_text), len(english_text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_0_sHOSM2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKSpwEASRvv"
      },
      "source": [
        "# data={'SMS_TEXT':sms_text,'ENGLISH_TEXT': english_text}\n",
        "# data=pd.DataFrame(data)\n",
        "# print(data.head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOHI3gpgDT4Y"
      },
      "source": [
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DghEGqZsHTD3"
      },
      "source": [
        "# data.to_csv('/content/drive/MyDrive/Applied Ai Course assignments/Assiginments/CS2/rawdata.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82u6VlUGHc6Z"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Applied Ai Course assignments/Assiginments/CS2/rawdata.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT3wku7cDWv0",
        "outputId": "1f03388b-8ce3-4b76-88e2-8d05b29e8ea9"
      },
      "source": [
        "print(\"Data Shape\",data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape (2000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cvj0T5dHndE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e8b696c1-87ca-480d-d598-b4421778cec6"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMS_TEXT</th>\n",
              "      <th>ENGLISH_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
              "      <td>Do you want me to reserve seat for you or not?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
              "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
              "      <td>They become more expensive already. Mine is li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I'm thai. what do u do?</td>\n",
              "      <td>I'm Thai. What do you do?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
              "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            SMS_TEXT                                       ENGLISH_TEXT\n",
              "0                    U wan me to \"chop\" seat 4 u nt?     Do you want me to reserve seat for you or not?\n",
              "1  Yup. U reaching. We order some durian pastry a...  Yeap. You reaching? We ordered some Durian pas...\n",
              "2  They become more ex oredi... Mine is like 25.....  They become more expensive already. Mine is li...\n",
              "3                            I'm thai. what do u do?                          I'm Thai. What do you do?\n",
              "4  Hi! How did your week go? Haven heard from you...  Hi! How did your week go? Haven't heard from y..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtkKiz9AH_gY"
      },
      "source": [
        "\n",
        "def preprocessing_steps(data):\n",
        "    \"\"\"Applying the length on both sms_text and english_text and filtering the sentences based on length \n",
        "    adding start token and end token for inputs and output dataframe\n",
        "    \\t-> start token which represents start of the sentence\n",
        "    \\n-> end token which represents end of the sentence.\n",
        "    Removing the sms_length, english_length, and ENGLISH_TEXT and appending ENGLISH_INPUT,ENGLISH_OUTPUT for the decoder.\"\"\"\n",
        "    data['sms_length']=data['SMS_TEXT'].apply(len)\n",
        "    data['eng_length']=data['ENGLISH_TEXT'].apply(len)\n",
        "    data=data[data['sms_length']<=170]\n",
        "    data=data[data['eng_length']<=200]\n",
        "    data['ENGLISH_INPUT']='\\t '+data['ENGLISH_TEXT'].astype(str)\n",
        "    data['ENGLISH_OUTPUT']=data['ENGLISH_TEXT'].astype(str)+' \\n'\n",
        "    data=data.drop(['sms_length','eng_length','ENGLISH_TEXT'],axis=1)\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aV6JUIKrxfi",
        "outputId": "3cb92029-c66c-423c-bd32-ccc3a8a486c7"
      },
      "source": [
        "preprocessed_data=preprocessing_steps(data)\n",
        "print(preprocessed_data.shape)\n",
        "preprocessed_data.iloc[0]['ENGLISH_INPUT']=str(preprocessed_data.iloc[0]['ENGLISH_INPUT'])+' \\n'\n",
        "preprocessed_data.iloc[0]['ENGLISH_OUTPUT']='\\t ' + str(preprocessed_data.iloc[0]['ENGLISH_OUTPUT'])+' \\n'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1993, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A_ydJK_8r1D2",
        "outputId": "707435c7-1007-46b6-e2f6-11e8aa7a4426"
      },
      "source": [
        "preprocessed_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMS_TEXT</th>\n",
              "      <th>ENGLISH_INPUT</th>\n",
              "      <th>ENGLISH_OUTPUT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
              "      <td>\\t Do you want me to reserve seat for you or n...</td>\n",
              "      <td>\\t Do you want me to reserve seat for you or n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
              "      <td>\\t Yeap. You reaching? We ordered some Durian ...</td>\n",
              "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
              "      <td>\\t They become more expensive already. Mine is...</td>\n",
              "      <td>They become more expensive already. Mine is li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I'm thai. what do u do?</td>\n",
              "      <td>\\t I'm Thai. What do you do?</td>\n",
              "      <td>I'm Thai. What do you do? \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
              "      <td>\\t Hi! How did your week go? Haven't heard fro...</td>\n",
              "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            SMS_TEXT  ...                                     ENGLISH_OUTPUT\n",
              "0                    U wan me to \"chop\" seat 4 u nt?  ...  \\t Do you want me to reserve seat for you or n...\n",
              "1  Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
              "2  They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
              "3                            I'm thai. what do u do?  ...                       I'm Thai. What do you do? \\n\n",
              "4  Hi! How did your week go? Haven heard from you...  ...  Hi! How did your week go? Haven't heard from y...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r1rQNa_sIWk",
        "outputId": "5612e510-d891-4f99-ddbf-44af1fb08b49"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data,test_data= train_test_split(preprocessed_data,test_size=0.01, random_state=42)\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1973, 3)\n",
            "(20, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvnBfck6sLVb",
        "outputId": "1a090e94-4e1b-43b6-b7f6-1a09bb20fff2"
      },
      "source": [
        "\n",
        "tokenizer = Tokenizer(filters=None,char_level=True,lower=False)\n",
        "print(\"SMS_TEXT\")\n",
        "tokenizer.fit_on_texts(train_data['SMS_TEXT'].values)\n",
        "print(\"English text\")\n",
        "tokenizer_e = Tokenizer(filters=None,char_level=True,lower=False)\n",
        "tokenizer_e.fit_on_texts(train_data['ENGLISH_INPUT'].values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMS_TEXT\n",
            "English text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXLBniTsOgF",
        "outputId": "38a97728-93f6-416d-cec5-7e0bb97c5d6d"
      },
      "source": [
        "encoder_vocab_size=len(tokenizer.word_index.keys())\n",
        "print(encoder_vocab_size)\n",
        "decoder_vocab_size=len(tokenizer_e.word_index.keys())\n",
        "print(decoder_vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103\n",
            "92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvGXhLtgsBGx"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdjHPdPxJavM"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_5NA24KzRp"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        \n",
        "      super().__init__()\n",
        "\n",
        "      self.e_embed = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length= input_length)\n",
        "      self.lstm_size = lstm_size\n",
        "      \n",
        "      self.e_lstm = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- encoder_output, last time step's hidden and cell state\n",
        "        '''\n",
        "        embedding = self.e_embed(input_sequence)\n",
        "        # print(embedding.shape)\n",
        "        # output_state, enc_h, enc_c = self.e_lstm( embedding, initial_state = states)\n",
        "        output_state, enc_h, enc_c = self.e_lstm( embedding)\n",
        "\n",
        "        return output_state, enc_h, enc_c\n",
        "\n",
        "      \n",
        "\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXn278lhLYRM"
      },
      "source": [
        "<font color='blue'>**Attention**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab5SNdPZLlur"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.scoring_function = scoring_function\n",
        "\n",
        "\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "\n",
        "    if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      self.dot= tf.keras.layers.Dot(axes = (1,2))      \n",
        "      pass\n",
        "    elif scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "      self.w= Dense(att_units)\n",
        "      self.dot = tf.keras.layers.Dot(axes = (1,2))\n",
        "      pass\n",
        "    elif scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      self.w1= Dense(att_units)\n",
        "      self.w2= Dense(att_units)\n",
        "      self.v= Dense(1)\n",
        "      pass\n",
        "  \n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "    '''\n",
        "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "        Multiply the score function with your encoder_outputs to get the context vector.\n",
        "        Function returns context vector and attention weights(softmax - scores)\n",
        "    '''\n",
        "    decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
        "    if self.scoring_function == 'dot':\n",
        "        # Implement Dot score function here\n",
        "        score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0,2,1)), encoder_output]), (0,2,1))\n",
        "        # print(\"In Dot\")\n",
        "\n",
        "        pass\n",
        "    elif self.scoring_function == 'general':\n",
        "        # Implement General score function here\n",
        "        # print(encoder_output.shape)\n",
        "        mulpy = self.w(encoder_output)\n",
        "        # print(mulpy.shape)\n",
        "        score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mulpy]), (0, 2,1),)\n",
        "\n",
        "        pass\n",
        "    elif self.scoring_function == 'concat':\n",
        "        inte = self.w1(decoder_hidden_state) + self.w2(encoder_output)\n",
        "        tan = tf.nn.tanh(inte)\n",
        "        score = self.v(tan)\n",
        "\n",
        "    att_weights = tf.nn.softmax(score, axis =1)\n",
        "    context_vector = att_weights * encoder_output\n",
        "    context_vector = tf.reduce_sum( context_vector, axis=1)\n",
        "    return context_vector, att_weights\n",
        "\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-FNEbfL2DN"
      },
      "source": [
        "<font color='blue'>**OneStepDecoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8m7lmOL097"
      },
      "source": [
        "class OneStepDecoder(tf.keras.Model):\n",
        "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "\n",
        "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "      super(OneStepDecoder, self).__init__()\n",
        "      self.dec_embed = Embedding(input_dim = tar_vocab_size, output_dim = embedding_dim)\n",
        "      self.lstm = LSTM(dec_units, return_sequences = True, return_state = True)\n",
        "      self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
        "      self.den = Dense(tar_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "    '''\n",
        "        One step decoder mechanisim step by step:\n",
        "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "      C. Concat the context vector with the step A output\n",
        "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "    '''\n",
        "    embd = self.dec_embed(input_to_decoder)\n",
        "    context_vec, attention_weights = self.attention( state_h,  encoder_output)    \n",
        "\n",
        "    f_inp = tf.concat([tf.expand_dims(context_vec, 1), embd], axis = -1)\n",
        "    # print(f_inp.shape)\n",
        "    out, dec_h, dec_c = self.lstm(f_inp, [state_h, state_c])\n",
        "    out = tf.reshape( out, (-1, out.shape[2]))\n",
        "    output = self.den(out)\n",
        "\n",
        "    return output, dec_h, dec_c, attention_weights, context_vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHrurjUMGAi"
      },
      "source": [
        "<font color='blue'>**Decoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-x31rj6Hc4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "      super(Decoder, self).__init__()\n",
        "      self.input_length = input_length\n",
        "      self.out_vocab_size = out_vocab_size\n",
        "      self.oneStepDecoder = OneStepDecoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "      self.out_vocab_size = out_vocab_size\n",
        "        \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        \n",
        "        #Iterate till the length of the decoder input\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            # Store the output in tensorarray\n",
        "        # Return the tensor array\n",
        "        # outputs = tf.TensorArray(dtype =  tf.float32, size= input_to_decoder.shape[1])\n",
        "        outputs = tf.TensorArray(dtype =  tf.float32, size= tf.shape(input_to_decoder)[1])\n",
        "\n",
        "        \n",
        "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
        "\n",
        "            output, decoder_hidden_state, decoder_cell_state, _, _ = self.oneStepDecoder(input_to_decoder[:, timestep:timestep+1],\n",
        "                                                                                          encoder_output,decoder_hidden_state,decoder_cell_state)                                                                                            \n",
        "                                                                                             \n",
        "                                                                                             \n",
        "            # Store the output in tensorarray\n",
        "            outputs = outputs.write(timestep, output)\n",
        "        # Return the tensor array\n",
        "        outputs = tf.transpose(outputs.stack(), (1, 0, 2))\n",
        "        return outputs\n",
        "        \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1T1EOoMTqC"
      },
      "source": [
        "<font color='blue'>**Encoder Decoder model**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfqBIe20MT3D"
      },
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "  def __init__(self,inp_vocab_size, out_vocab_size,input_length, enc_units,embedding_dim, dec_units, max_len, score_fun, att_units, batch_size):\n",
        "    #Intialize objects from encoder decoder\n",
        "    super(encoder_decoder, self).__init__()\n",
        "    self.encoder = Encoder(inp_vocab_size= inp_vocab_size +1, embedding_size=embedding_dim, lstm_size= att_units,input_length= max_len)\n",
        "    self.decoder = Decoder(out_vocab_size +1, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  \n",
        "  def call(self,data):\n",
        "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "    # return the decoder output\n",
        "    e_inp, d_inp = data[0], data[1]\n",
        "    # print(data[0].shape)\n",
        "    # print(data[1].shape)\n",
        "\n",
        "\n",
        "    initial_state = self.encoder.initialize_states(self.batch_size)\n",
        "\n",
        "    e_output, enc_h, enc_c = self.encoder(e_inp,initial_state)\n",
        "    outputs = tf.TensorArray(dtype = tf.float32, size= 20)\n",
        "        \n",
        "    dec_h = enc_h\n",
        "    dec_c = enc_c\n",
        "    # print(dec_h.shape)\n",
        "    output=self.decoder(d_inp,e_output, dec_h, dec_c)\n",
        "  \n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRxB-FDMJWL"
      },
      "source": [
        "<font color='blue'>**Custom loss function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY_3izrXMs8y"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
        "\n",
        "def custom_lossfunction(targets,logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  loss_ = loss_object(targets, logits)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "# def custom_lossfunction(targets,logits):\n",
        "\n",
        "  # Custom loss function that will not consider the loss for padded zeros.\n",
        "  # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QlbWAqNNlqe"
      },
      "source": [
        "<font color='blue'>**Training**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNRsc0GPuaUC"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, tknizer_ita, tknizer_eng, max_len,max_len1):\n",
        "        self.encoder_inps = data['SMS_TEXT'].values\n",
        "        self.decoder_inps = data['ENGLISH_INPUT'].values\n",
        "        self.decoder_outs = data['ENGLISH_OUTPUT'].values\n",
        "        self.tknizer_eng = tknizer_eng\n",
        "        self.tknizer_ita = tknizer_ita\n",
        "        self.max_len = max_len\n",
        "        self.max_len1=max_len1\n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
        " \n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len1, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len1, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        " \n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        " \n",
        "    \n",
        "class Dataloder(tf.keras.utils.Sequence):    \n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        " \n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        " \n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "        return tuple([[batch[0],batch[1]],batch[2]])\n",
        " \n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        " \n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtYqvcqvufUH"
      },
      "source": [
        "train_dataset = Dataset(train_data, tokenizer, tokenizer_e, 170,202)\n",
        "test_dataset  = Dataset(test_data, tokenizer, tokenizer_e, 170,202)\n",
        " \n",
        "train_dataloader = Dataloder(train_dataset, batch_size=64)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMyAx1q7vT5e"
      },
      "source": [
        "lstm_size = 100\n",
        "embedding_dim =128\n",
        "att_units = 100\n",
        "dec_units=100\n",
        "batch_size = 64 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thk0nRrjuhV8",
        "outputId": "bf60e422-3345-4577-e040-41a48bf961fa"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model2  = encoder_decoder(encoder_vocab_size,decoder_vocab_size,170,lstm_size,embedding_dim,att_units, 202,'concat',dec_units,batch_size)\n",
        "optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "model2.compile(optimizer=optimizer,loss=custom_lossfunction)\n",
        "train_steps=train_data.shape[0]//64\n",
        "valid_steps=test_data.shape[0]//20\n",
        "model2.fit(train_dataloader, steps_per_epoch=train_steps, epochs=100, validation_data=test_dataloader, validation_steps=valid_steps)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 65s 2s/step - loss: 1.1586 - val_loss: 1.1456\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.0008 - val_loss: 1.0708\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.9535 - val_loss: 1.0245\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.9182 - val_loss: 0.9874\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.8876 - val_loss: 0.9547\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.8592 - val_loss: 0.9215\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.8338 - val_loss: 0.8973\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.8136 - val_loss: 0.8782\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7959 - val_loss: 0.8616\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7807 - val_loss: 0.8495\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7663 - val_loss: 0.8330\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7541 - val_loss: 0.8234\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7396 - val_loss: 0.8098\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7266 - val_loss: 0.7974\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7128 - val_loss: 0.7903\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.6989 - val_loss: 0.7855\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.6818 - val_loss: 0.7641\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.6657 - val_loss: 0.7493\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.6440 - val_loss: 0.7248\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.6143 - val_loss: 0.6923\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.5864 - val_loss: 0.6609\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.5481 - val_loss: 0.6331\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.5055 - val_loss: 0.5966\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.4728 - val_loss: 0.5857\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.4449 - val_loss: 0.5603\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.4150 - val_loss: 0.5280\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.4047 - val_loss: 0.5546\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.3766 - val_loss: 0.4798\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.3499 - val_loss: 0.4848\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.3569 - val_loss: 0.4755\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.3578 - val_loss: 0.4409\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3415 - val_loss: 0.4596\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3229 - val_loss: 0.4219\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3025 - val_loss: 0.4243\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3064 - val_loss: 0.4247\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2918 - val_loss: 0.4098\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2798 - val_loss: 0.3929\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2622 - val_loss: 0.3920\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2538 - val_loss: 0.3731\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2362 - val_loss: 0.3881\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2383 - val_loss: 0.3856\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2340 - val_loss: 0.4025\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2867 - val_loss: 0.4983\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.3474 - val_loss: 0.4517\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2895 - val_loss: 0.3989\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2649 - val_loss: 0.4282\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3492 - val_loss: 0.4337\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2897 - val_loss: 0.4001\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2794 - val_loss: 0.4143\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2679 - val_loss: 0.4157\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2505 - val_loss: 0.4004\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2300 - val_loss: 0.3540\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2162 - val_loss: 0.3545\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2409 - val_loss: 0.3773\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2226 - val_loss: 0.3761\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2253 - val_loss: 0.3829\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2267 - val_loss: 0.3720\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2148 - val_loss: 0.3708\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.2022 - val_loss: 0.3501\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.1863 - val_loss: 0.3590\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.1787 - val_loss: 0.3672\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.1761 - val_loss: 0.3730\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.1656 - val_loss: 0.3561\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1577 - val_loss: 0.3578\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1555 - val_loss: 0.3835\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1528 - val_loss: 0.3588\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1544 - val_loss: 0.3478\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1495 - val_loss: 0.3876\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1612 - val_loss: 0.3903\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.1966 - val_loss: 0.3997\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2056 - val_loss: 0.3823\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2368 - val_loss: 0.5418\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3631 - val_loss: 0.5548\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.4306 - val_loss: 0.5322\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.4000 - val_loss: 0.4533\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3039 - val_loss: 0.4620\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3996 - val_loss: 1.0778\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.9034 - val_loss: 0.8967\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.7263 - val_loss: 0.7542\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.5998 - val_loss: 0.6521\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.5008 - val_loss: 0.5734\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.4403 - val_loss: 0.5189\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.4001 - val_loss: 0.4733\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3622 - val_loss: 0.4651\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3448 - val_loss: 0.4375\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3171 - val_loss: 0.4496\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3082 - val_loss: 0.4413\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3011 - val_loss: 0.4503\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3669 - val_loss: 0.5227\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3049 - val_loss: 0.4334\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2745 - val_loss: 0.3943\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2581 - val_loss: 0.3995\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2421 - val_loss: 0.3935\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2327 - val_loss: 0.3904\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3693 - val_loss: 0.4549\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.3143 - val_loss: 0.4386\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2713 - val_loss: 0.3969\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2466 - val_loss: 0.3828\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2270 - val_loss: 0.3867\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 56s 2s/step - loss: 0.2160 - val_loss: 0.3895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc44552e390>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya4OGUvumHG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm3ENXi9JFFU"
      },
      "source": [
        "Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0y95822Gy0X"
      },
      "source": [
        "# https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "  sequences = [[list(), 0.0]]\n",
        "  # print(\"asd\")\n",
        "  for row in data:\n",
        "    all_candidates = list()\n",
        "    for i in range(len(sequences)):\n",
        "      seq, score = sequences[i]\n",
        "      for j in range(len(row)):\n",
        "        try:\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "        except ValueError as e:\n",
        "          candidate = [seq + [j], 0]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    # select k best\n",
        "    sequences = ordered[:k]\n",
        "  return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRfqoeigGy0X"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "def evaluate(sentence):\n",
        "  max_length_targ = 202\n",
        "  max_length_inp  = 170\n",
        "  \n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "  # sentence = preprocess(sentence)\n",
        "  sentence = sentence.strip()\n",
        "  #pdb.set_trace()\n",
        "  inputs = [] \n",
        "  for word in sentence:\n",
        "      inputs.append(tokenizer.word_index[word])\n",
        "  inputs =  tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post') \n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = '' \n",
        "\n",
        "  initial_state=model2.layers[0].initialize_states(batch_size=1)\n",
        "  encoder_outputs, state_h,state_c = model2.layers[0](inputs,initial_state)   \n",
        "  # dec_input = tf.expand_dims([tokenizer_e.word_index['<start>']], 0)\n",
        "  dec_input=tf.expand_dims([tokenizer_e.word_index['\\t']],0)\n",
        "\n",
        "\n",
        "  for i in range(max_length_targ):\n",
        "    Output,state_h,state_c,att_weights,_ = model2.layers[1].oneStepDecoder(dec_input,encoder_outputs,state_h,state_c)\n",
        "    #Beam Search Decoder\n",
        "    Result_beam_list=beam_search_decoder(Output,k=1)\n",
        "    Result_beam=Result_beam_list[0][0]\n",
        "\n",
        "    attention_weights = tf.reshape(att_weights, (-1, ))\n",
        "    attention_plot[i]  = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(Output[0]).numpy()\n",
        "  \n",
        "    result += tokenizer_e.index_word[Result_beam[0]] + ' '\n",
        "    if tokenizer_e.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXwRSoajGy0X",
        "outputId": "33e655c7-1426-407c-9ccb-e11d3940999d"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from tqdm import tqdm\n",
        "def BleuScore(validation):\n",
        "    input=list(validation['SMS_TEXT'])\n",
        "    Y_true=list(validation['ENGLISH_OUTPUT'])\n",
        "    results = []\n",
        "    bleuscores=[]\n",
        "    input_sent_list = []\n",
        "    # print(len(input))\n",
        "    for i in tqdm(range(len(input))):\n",
        "      try:\n",
        "        # print(\"1\")\n",
        "        result, sentence, attention_plot = evaluate(input[i])\n",
        "        # print(result)\n",
        "        \n",
        "        results.append(result)  \n",
        "        input_sent_list.append(sentence)    \n",
        "        bleuscores.append(bleu.sentence_bleu(Y_true[i], result))\n",
        "        # print(\":sss\")\n",
        "      except KeyError as e:\n",
        "        pass  \n",
        "    return sum(bleuscores)/len(bleuscores),bleuscores,results,input_sent_list\n",
        "\n",
        "AvearageScore,bleuscores,results,input_sent_list=BleuScore(test_data)\n",
        "\n",
        "print(\"Average Bleuscore:\",AvearageScore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [03:43<00:00, 11.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Bleuscore: 0.4491888111793926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YvYwwZnbhWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DpC9zlzMcXp"
      },
      "source": [
        "## <font color='blue'>**Inference**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CYQBfHJXDQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK-ewNkfXDS0"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    print(\"-\"*50)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRcD2rL1gaYg",
        "outputId": "2afa1d3a-8fc4-4d73-96f7-65a0958f6e3c"
      },
      "source": [
        "test_data[\"SMS_TEXT\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "891     Ya...Dat dae i check dun haf...So how? Where u...\n",
              "1676    I thk u can, juz get e add from ur frens first...\n",
              "415                                 Oh, wanna go there ma\n",
              "1602                               Ü driving there tmr...\n",
              "851     U go chop seats in e canteen b4 2 ok... Den i ...\n",
              "1662    really? yep i'll probably see him in camp. tmr...\n",
              "928     Hey xin ah...R we goin 4 lesson on thurs? Oh f...\n",
              "1674    Freshman Orientatn Week. It starts tis fri,if ...\n",
              "1451    Hey yijue how are u getting there later.... We...\n",
              "1219    Hey ü wanna meet outside e lt? Haha... Time's ...\n",
              "451                    Joey: HELLO R U GUY OR GAL? ME GAL\n",
              "487     My phone no batt. Pick me up at 2pm at drive o...\n",
              "1986    My sis so bo liao u noe. She arguing w me abt ...\n",
              "934     Bad news... I forgot put my cash card when ent...\n",
              "1308          545 can... Cos i finish work at tis time...\n",
              "1001    Ay paiseh din check my fone yest.btw,u free la...\n",
              "124     Just came to nydc n she just ordered a baked r...\n",
              "1431    Hey... I go suntec n find u... Wait 4 me... I'...\n",
              "553                  Yup... I will be going with my hall.\n",
              "582                                  Sen, u male o female\n",
              "Name: SMS_TEXT, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhc7SFLDXDVM",
        "outputId": "b4d57994-43ad-4fbb-d413-329a55fb92d1"
      },
      "source": [
        "result=translate(test_data[\"SMS_TEXT\"][891])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Ya...Dat dae i check dun haf...So how? Where u wan?\n",
            "Predicted translation: Y S   e t e d a   h r . \n",
            " o   o   a e h r a S . S   u t s e   o   a e a s   o   a e a s   o   o e ? W   o   a e ? W   o   a e ? W   o   o   o   o e ? \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm_ClYJXgfSv",
        "outputId": "0980fad7-3100-49ff-cb3c-581aa2d7140a"
      },
      "source": [
        "result=translate(test_data[\"SMS_TEXT\"][1676])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I thk u can, juz get e add from ur frens first lor... Haha, i did a lot of shoppin. Felt quite bad, cos aust shop a lot oredi...\n",
            "Predicted translation: I I   u   l n g   o     o   o d   e t   e t   e t   e t   e t   e t   e t   f t   o   o e   o e f r o s   i   n t   i   n t   i   n t   i   a d i   o   s o p i n   o   s o p i n   o   s e p   n t   u t t r f t e   o   o d t   f r e   o y t l   c u t   o t e   o   o t e   o   o t e   o   o t d   l t   a   o t d   l s   a d o t   s o p a   a d o t   s o p     l c a s e   o   o t d   l t   a e o l   q a \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YykG05eWXDXp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1IhdBrgQYJr"
      },
      "source": [
        "<font color='blue'>**Predict the sentence translation**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDT91Im6MZS6"
      },
      "source": [
        "\n",
        "def predict(input_sentence):\n",
        "\n",
        "  '''\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "  C. Initialize index of <\\t> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
        "  D. till we reach max_length of decoder or till the model predicted character <\\n>:\n",
        "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
        "         And get the character using the tokenizer(character index) and then store it in a string.\n",
        "  E. Return the predicted sentence\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  input_sequence=tokenizer.texts_to_sequences([input_sentence])\n",
        "  \n",
        "\n",
        "  inputs=pad_sequences(input_sequence,maxlen=170,padding='post')\n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "  result=''\n",
        "  units=100\n",
        "  hidden=[tf.zeros((1,units))]\n",
        "  encoder_output,hidden_state,cell_state=model2.layers[0](inputs,hidden)\n",
        "  dec_hidden=hidden_state\n",
        "  dec_input=tf.expand_dims([tokenizer_e.word_index['\\t']],0)\n",
        "  for t in range(202):\n",
        "      predictions,dec_hidden,cell_state,attention_weights,context_vector=model2.layers[1].oneStepDecoder(dec_input,encoder_output,dec_hidden,cell_state)\n",
        "\n",
        "      predicted_id=tf.argmax(predictions[0]).numpy()\n",
        "      result+=tokenizer_e.index_word[predicted_id]\n",
        "      if tokenizer_e.word_index['\\n']==predicted_id:\n",
        "          return result\n",
        "      dec_input= tf.expand_dims([predicted_id],0)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyFvVCuQOQ5J"
      },
      "source": [
        "max_len = 170"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z-As25YMd47",
        "outputId": "b52f6e96-b714-4cae-e18b-6161c9788476"
      },
      "source": [
        "for index in range(len(test_data)):\n",
        "    print('*'*200)\n",
        "    print(\"SMS_TEXT\")\n",
        "    print(test_data['SMS_TEXT'].values[index])\n",
        "    print(\"Prediction\")\n",
        "    print(predict(test_data['SMS_TEXT'].values[index]))\n",
        "    print(\"Original Sentence\")\n",
        "    print(test_data['ENGLISH_OUTPUT'].iloc[index])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Ya...Dat dae i check dun haf...So how? Where u wan?\n",
            "Prediction\n",
            "YS eteda hr.\n",
            "\n",
            "Original Sentence\n",
            "Yeah. That day I checked, did not have. So how? Where do you want? \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "I thk u can, juz get e add from ur frens first lor... Haha, i did a lot of shoppin. Felt quite bad, cos aust shop a lot oredi...\n",
            "Prediction\n",
            "II u lng o  o od et et et et et et et ft o oe oefros i nt i nt i nt i adi o sopin o sopin o sep nt uttrfte o odt fre oytl cut ote o ote o ote o otd lt a otd ls adot sopa adot sop  lcase o otd lt aeol qa\n",
            "Original Sentence\n",
            "I think you can, just get the address from your friends first. Haha, I did a lot of shopping. Felt quite bad, because Aust shop a lot already. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Oh, wanna go there ma\n",
            "Prediction\n",
            "OOl o t nt o aetraey i nt u aetraey o?\n",
            "\n",
            "Original Sentence\n",
            "Oh, want to go there? \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Ü driving there tmr...\n",
            "Prediction\n",
            "Yyyyu nwwrai nter oe.\n",
            "\n",
            "Original Sentence\n",
            "Are you driving there tomorrow? \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "U go chop seats in e canteen b4 2 ok... Den i come out can eat oredi...\n",
            "Prediction\n",
            "Yyu o o oe haes i te eaes eo  o of o of o oe  tetriediedtroto ae lcol?D etn lt o ote oedi.\n",
            "\n",
            "Original Sentence\n",
            "You occupy seats in the canteen before 2. Then I come out can eat already. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "really? yep i'll probably see him in camp. tmr's all company g\n",
            "Prediction\n",
            "RHp.?Y leb o uyn so ao ho hv ho smp lma rmp lklcas tems lkp.T i al cmpany hnwsae.\n",
            "\n",
            "Original Sentence\n",
            "Really? Yes, I'll probably see him in camp. Tomorrow is all company going. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Hey xin ah...R we goin 4 lesson on thurs? Oh fri rite, we r attendin e theory lesson hor...\n",
            "Prediction\n",
            "Heh eih.AS rgin o leson o tei?\n",
            "\n",
            "Original Sentence\n",
            "Hey, Xin. Are we going for lesson on Thursday? Oh, Friday night we are attending the theory lesson? \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Freshman Orientatn Week. It starts tis fri,if u reali reali want to join,u cum down to sch,dere will b pple ard,ask ask lor.\n",
            "Prediction\n",
            "Fresmn o enntatnttten hrp.Itdtin hrpt ta sart hs u lsl,we i o oe o nt u oih o oih o oen o omndywy hw o ol eol o al ak o al ak lkn al a al ak lkol.\n",
            "\n",
            "Original Sentence\n",
            "Freshman Orientation Week. It starts this Friday, if you really really want to join, you can come down to school. There will be people around, just ask. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Hey yijue how are u getting there later.... We are meeting at 730 orchard mrt.\n",
            "Prediction\n",
            "Heh yjs hw aeey o hvtlnae hnee etin teresae.We o ae ae ra reer ee ae ae2 o ae o hr.H o hrw rcorn ot.\n",
            "\n",
            "Original Sentence\n",
            "Hey Yijue, how are you getting there later? We are meeting at 7:30 at Orchard MRT. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Hey ü wanna meet outside e lt? Haha... Time's a little tight though...\n",
            "Prediction\n",
            "Heh o oe o soe otside ea?\n",
            "\n",
            "Original Sentence\n",
            "Hey, do you want to meet outside the lecture theatre? Haha. Time's a little tight though. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Joey: HELLO R U GUY OR GAL? ME GAL\n",
            "Prediction\n",
            "JOZ o o o o o o o aetrte o nge o ol o aleaas o ol eee o aleay ol alesae.\n",
            "\n",
            "Original Sentence\n",
            "Joey: Hello, are you a boy or a girl? I'm a girl. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "My phone no batt. Pick me up at 2pm at drive one. Have you all eaten? Yun here using huixin's phone...\n",
            "Prediction\n",
            "Mypwypy o ote.Pigt o at2 o a ae2p epate a oe a oe aloe a oe hvlyee nvny al eten?Y i oe hur uin hreeuy soxin hoe uin hoeiu hre uinihse o inih o inih o inih o in songa.\n",
            "\n",
            "Original Sentence\n",
            "My phone has no battery. Pick me up at 2PM at Drive. Have you all eaten? Faint, here is using Huixin's phone. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "My sis so bo liao u noe. She arguing w me abt sun's hair den accuse me of smethg i haven thgt of. Hate it when pple accuse me...\n",
            "Prediction\n",
            "Mya s es u a us i o aeuty o nt.Its so o aeuii o a aeuty u a ae o alesas so' hi snts hv adin i nt u oesmto ae o alesae.H.H a uv oe elesae oe olas see o oe poe elescm ecuae.\n",
            "\n",
            "Original Sentence\n",
            "My sister does silly things you know. She was arguing with me about Sun's hair and then she accuse me of something I haven't thought of. I hate it when people accuse me. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Bad news... I forgot put my cash card when entering erp...\n",
            "Prediction\n",
            "BJbroe i oe oeagy i oe ad a ae o csh o ase hrnwrnwrigtrig et.R ar o alee oter ote o aee o aee' et.\n",
            "\n",
            "Original Sentence\n",
            "Bad news. I forgot to put my cash card in when entering ERP. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "545 can... Cos i finish work at tis time...\n",
            "Prediction\n",
            "Fe5 o al a i nt se lciec i nt i nt i tie.Iwy o oe  orw o ali omtriy omtrin.\n",
            "\n",
            "Original Sentence\n",
            "At 5:45 I can. Because I finish work at this time. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Ay paiseh din check my fone yest.btw,u free later?wana visit fion?\n",
            "Prediction\n",
            "A,Oal,I i ned.I hv ed.I hv o fne yu fne yu fee lter?Iwy o oee o aea rgia otvrin fon?\n",
            "\n",
            "Original Sentence\n",
            "Sorry, didn't check my phone yesterday. By the way, are you free later? Want to visit Fion? \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Just came to nydc n she just ordered a baked rice n i ordered a drink. U done liao.\n",
            "Prediction\n",
            "JJjIJ o adcuen nwce netrouh nwceed o eke a erere rgede rgede ar o ed eoe adrdy o etek  u o odesae aruno oe oee o ode.I se oe oee o ode.I se oe oee o ed o ode.\n",
            "\n",
            "Original Sentence\n",
            "Just came to nydc, she just ordered a baked rice and I ordered a drink. You done already. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Hey... I go suntec n find u... Wait 4 me... I'm on my way liao...\n",
            "Prediction\n",
            "Heh i o sotec nxto oemetod i o oe  a oe.I mnw n m nw ea o m nw as lcon.\n",
            "\n",
            "Original Sentence\n",
            "Hey, I'll go Suntec and find you. Wait for me. I'm on my way. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Yup... I will be going with my hall.\n",
            "Prediction\n",
            "YKy i o eih i oih o hlie o hll ae o hll ae.\n",
            "\n",
            "Original Sentence\n",
            "Yes. I will be going with my hall. \n",
            "\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Sen, u male o female\n",
            "Prediction\n",
            "SS o ol omal o am omal o am omal eo omale ar omae i nt u umueay o aleaee o aleay o aleaee o ales o ales o ales o aless u a aesusss aeessss aeesss aeessus adsas uls uls uls uls uls uls uls uls uls uls ul\n",
            "Original Sentence\n",
            "Sen, are you male or female? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BdPFxK6Pasd",
        "outputId": "2fdbd46a-3680-48b4-ed0b-cf173f9cb8d5"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "sum=0\n",
        "for index in range(len(test_data)):\n",
        "    reference=test_data['ENGLISH_OUTPUT'].iloc[index]\n",
        "    translation=predict(test_data['SMS_TEXT'].iloc[index])\n",
        "    sum+=bleu.sentence_bleu(reference,translation)\n",
        "print(sum/len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6780770304835106\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}