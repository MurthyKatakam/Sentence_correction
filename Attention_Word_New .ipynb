{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attention_Word_New.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3u462KOCgVC"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,GRU,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "import six\n",
        "from tensorflow.keras.utils import deserialize_keras_object\n",
        "from joblib import dump, load\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,GRU,Embedding,Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow.keras.layers import concatenate, Lambda\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UylqAaZMgCqT"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbpUygy06BEx",
        "outputId": "b0fb2c82-4ee8-49fe-d1da-6df7b5b631b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLVakVWSHXT"
      },
      "source": [
        "\n",
        "# def preparing_data(file_name):\n",
        "#     files=open(file_name,'r')\n",
        "#     sentences=files.readlines()\n",
        "#     files.close()\n",
        "#     # print(lines)\n",
        "#     sms_text=[]\n",
        "#     english_text=[]\n",
        "\n",
        "#     for index,sentence in enumerate(sentences):\n",
        "#         if index%3==0:\n",
        "#            sms_text.append(sentence.strip())\n",
        "#         elif index%3==1:\n",
        "#             english_text.append(sentence.strip())\n",
        "#         else:\n",
        "#             pass\n",
        "#     return sms_text,english_text             \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZpVAyljSLex"
      },
      "source": [
        "# file_name='/content/drive/MyDrive/Applied Ai Course assignments/Assiginments/CS2/en2cn-2k.en2nen2cn'\n",
        "# sms_text,english_text=preparing_data(file_name)\n",
        "# print(len(sms_text), len(english_text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_0_sHOSM2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKSpwEASRvv"
      },
      "source": [
        "# data={'SMS_TEXT':sms_text,'ENGLISH_TEXT': english_text}\n",
        "# data=pd.DataFrame(data)\n",
        "# print(data.head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOHI3gpgDT4Y"
      },
      "source": [
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82u6VlUGHc6Z"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Applied Ai Course assignments/sentence_correction/rawdata.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT3wku7cDWv0",
        "outputId": "76967974-1e1a-48b0-cadc-e093b15b0692"
      },
      "source": [
        "print(\"Data Shape\",data.shape)      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape (2000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cvj0T5dHndE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "426136b0-a6a0-4aa4-9e67-ed5cd870318e"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMS_TEXT</th>\n",
              "      <th>ENGLISH_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
              "      <td>Do you want me to reserve seat for you or not?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
              "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
              "      <td>They become more expensive already. Mine is li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I'm thai. what do u do?</td>\n",
              "      <td>I'm Thai. What do you do?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
              "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            SMS_TEXT                                       ENGLISH_TEXT\n",
              "0                    U wan me to \"chop\" seat 4 u nt?     Do you want me to reserve seat for you or not?\n",
              "1  Yup. U reaching. We order some durian pastry a...  Yeap. You reaching? We ordered some Durian pas...\n",
              "2  They become more ex oredi... Mine is like 25.....  They become more expensive already. Mine is li...\n",
              "3                            I'm thai. what do u do?                          I'm Thai. What do you do?\n",
              "4  Hi! How did your week go? Haven heard from you...  Hi! How did your week go? Haven't heard from y..."
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtkKiz9AH_gY"
      },
      "source": [
        "\n",
        "def preprocessing_steps(data):\n",
        "    \"\"\"Applying the length on both sms_text and english_text and filtering the sentences based on length \n",
        "    adding start token and end token for inputs and output dataframe\n",
        "    \\t-> start token which represents start of the sentence\n",
        "    \\n-> end token which represents end of the sentence.\n",
        "    Removing the sms_length, english_length, and ENGLISH_TEXT and appending ENGLISH_INPUT,ENGLISH_OUTPUT for the decoder.\"\"\"\n",
        "    data['sms_length']=data['SMS_TEXT'].str.split().apply(len)\n",
        "    data['eng_length']=data['ENGLISH_TEXT'].str.split().apply(len)\n",
        "    data=data[data['sms_length']<=39]\n",
        "    data=data[data['eng_length']<=40]\n",
        "    data['ENGLISH_INPUT']='<start> '+data['ENGLISH_TEXT'].astype(str)\n",
        "    data['ENGLISH_OUTPUT']=data['ENGLISH_TEXT'].astype(str)+' <end>'\n",
        "    data=data.drop(['sms_length','eng_length','ENGLISH_TEXT'],axis=1)\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aV6JUIKrxfi",
        "outputId": "76a11896-3913-4966-f615-05373e73f4fd"
      },
      "source": [
        "preprocessed_data=preprocessing_steps(data)\n",
        "print(preprocessed_data.shape)\n",
        "preprocessed_data.iloc[0]['ENGLISH_INPUT']=str(preprocessed_data.iloc[0]['ENGLISH_INPUT'])+' <end>'\n",
        "# preprocessed_data.iloc[0]['ENGLISH_OUTPUT']='\\t ' + str(preprocessed_data.iloc[0]['ENGLISH_OUTPUT'])+' \\n'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1991, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A_ydJK_8r1D2",
        "outputId": "f8ee4488-29d7-40d6-81a4-60beb4a69bf2"
      },
      "source": [
        "preprocessed_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMS_TEXT</th>\n",
              "      <th>ENGLISH_INPUT</th>\n",
              "      <th>ENGLISH_OUTPUT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
              "      <td>&lt;start&gt; Do you want me to reserve seat for you...</td>\n",
              "      <td>Do you want me to reserve seat for you or not?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
              "      <td>&lt;start&gt; Yeap. You reaching? We ordered some Du...</td>\n",
              "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
              "      <td>&lt;start&gt; They become more expensive already. Mi...</td>\n",
              "      <td>They become more expensive already. Mine is li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I'm thai. what do u do?</td>\n",
              "      <td>&lt;start&gt; I'm Thai. What do you do?</td>\n",
              "      <td>I'm Thai. What do you do? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
              "      <td>&lt;start&gt; Hi! How did your week go? Haven't hear...</td>\n",
              "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            SMS_TEXT  ...                                     ENGLISH_OUTPUT\n",
              "0                    U wan me to \"chop\" seat 4 u nt?  ...  Do you want me to reserve seat for you or not?...\n",
              "1  Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
              "2  They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
              "3                            I'm thai. what do u do?  ...                    I'm Thai. What do you do? <end>\n",
              "4  Hi! How did your week go? Haven heard from you...  ...  Hi! How did your week go? Haven't heard from y...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r1rQNa_sIWk",
        "outputId": "79f4bef1-08ad-45e8-9f5b-af40363d5fe7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data,test_data= train_test_split(preprocessed_data,test_size=0.01, random_state=42)\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1971, 3)\n",
            "(20, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2qstuEo4KgK7",
        "outputId": "50ea1d2c-70b9-46c1-c5cf-d3918c8b6227"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMS_TEXT</th>\n",
              "      <th>ENGLISH_INPUT</th>\n",
              "      <th>ENGLISH_OUTPUT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>Leaving ard that time too.  Bringing laptop home?</td>\n",
              "      <td>&lt;start&gt; Leaving around that time too. Bringing...</td>\n",
              "      <td>Leaving around that time too. Bringing laptop ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1974</th>\n",
              "      <td>Huh. If wont finish today that means i will pr...</td>\n",
              "      <td>&lt;start&gt; Huh? If I won't finish today, that mea...</td>\n",
              "      <td>Huh? If I won't finish today, that means I wil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>Ya la...No choice...Wat to do? U intro me some...</td>\n",
              "      <td>&lt;start&gt; Yes. No choice. What to do? You can in...</td>\n",
              "      <td>Yes. No choice. What to do? You can introduce ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1671</th>\n",
              "      <td>Hi, may i know ur handphone number</td>\n",
              "      <td>&lt;start&gt; Hi, may I know your handphone number?</td>\n",
              "      <td>Hi, may I know your handphone number? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>Hey yun... Can you help me print something out...</td>\n",
              "      <td>&lt;start&gt; Hey Yun.  Can you help me print someth...</td>\n",
              "      <td>Hey Yun.  Can you help me print something out ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1133</th>\n",
              "      <td>Haha.... Ya i don mind... U are not right?</td>\n",
              "      <td>&lt;start&gt; Haha. I don't mind. You are not right?</td>\n",
              "      <td>Haha. I don't mind. You are not right? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1298</th>\n",
              "      <td>Blk 295 punggol central.</td>\n",
              "      <td>&lt;start&gt; Block 295 Punggol Central.</td>\n",
              "      <td>Block 295 Punggol Central. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>Its ok. Hi ranger u m or f? Workg? Age? Its ok...</td>\n",
              "      <td>&lt;start&gt; It's ok. Hi, ranger you male or female...</td>\n",
              "      <td>It's ok. Hi, ranger you male or female? Workin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>Hey yun ask you ah... where did you the answer...</td>\n",
              "      <td>&lt;start&gt; Hey Yun, can I ask you? Where did you ...</td>\n",
              "      <td>Hey Yun, can I ask you? Where did you get the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>Anyone free today?Wanna go ecp?or expo?</td>\n",
              "      <td>&lt;start&gt; Anyone free today? Want to go to ecp? ...</td>\n",
              "      <td>Anyone free today? Want to go to ecp? or expo?...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1971 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               SMS_TEXT  ...                                     ENGLISH_OUTPUT\n",
              "396   Leaving ard that time too.  Bringing laptop home?  ...  Leaving around that time too. Bringing laptop ...\n",
              "1974  Huh. If wont finish today that means i will pr...  ...  Huh? If I won't finish today, that means I wil...\n",
              "516   Ya la...No choice...Wat to do? U intro me some...  ...  Yes. No choice. What to do? You can introduce ...\n",
              "1671                 Hi, may i know ur handphone number  ...        Hi, may I know your handphone number? <end>\n",
              "351   Hey yun... Can you help me print something out...  ...  Hey Yun.  Can you help me print something out ...\n",
              "...                                                 ...  ...                                                ...\n",
              "1133         Haha.... Ya i don mind... U are not right?  ...       Haha. I don't mind. You are not right? <end>\n",
              "1298                           Blk 295 punggol central.  ...                   Block 295 Punggol Central. <end>\n",
              "862   Its ok. Hi ranger u m or f? Workg? Age? Its ok...  ...  It's ok. Hi, ranger you male or female? Workin...\n",
              "1463  Hey yun ask you ah... where did you the answer...  ...  Hey Yun, can I ask you? Where did you get the ...\n",
              "1129            Anyone free today?Wanna go ecp?or expo?  ...  Anyone free today? Want to go to ecp? or expo?...\n",
              "\n",
              "[1971 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvnBfck6sLVb",
        "outputId": "a35fcf08-e72c-44c1-b466-0dd0992889d8"
      },
      "source": [
        "\n",
        "tokenizer = Tokenizer(filters=\" \",char_level=False,lower=False)\n",
        "print(\"SMS_TEXT\")\n",
        "tokenizer.fit_on_texts(train_data['SMS_TEXT'].values)\n",
        "print(\"English text\")\n",
        "tokenizer_e = Tokenizer(filters=\" \",char_level=False,lower=False)\n",
        "tokenizer_e.fit_on_texts(train_data['ENGLISH_INPUT'].values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMS_TEXT\n",
            "English text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXLBniTsOgF",
        "outputId": "58e2f526-4d01-4214-9c85-2178173f8a65"
      },
      "source": [
        "encoder_vocab_size=len(tokenizer.word_index.keys())\n",
        "print(encoder_vocab_size)\n",
        "decoder_vocab_size=len(tokenizer_e.word_index.keys())\n",
        "print(decoder_vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6943\n",
            "5266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kkKj7AfcGAU",
        "outputId": "8d1a5335-357a-4ea6-f8ab-04adc7586336"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'u': 1,\n",
              " 'to': 2,\n",
              " 'i': 3,\n",
              " 'I': 4,\n",
              " 'at': 5,\n",
              " 'go': 6,\n",
              " 'a': 7,\n",
              " 'me': 8,\n",
              " 'ü': 9,\n",
              " 'my': 10,\n",
              " 'the': 11,\n",
              " 'e': 12,\n",
              " 'for': 13,\n",
              " 'in': 14,\n",
              " 'U': 15,\n",
              " 'got': 16,\n",
              " 'can': 17,\n",
              " 'you': 18,\n",
              " 'is': 19,\n",
              " 'on': 20,\n",
              " 'ur': 21,\n",
              " '2': 22,\n",
              " 'not': 23,\n",
              " 'of': 24,\n",
              " 'n': 25,\n",
              " 'so': 26,\n",
              " '4': 27,\n",
              " 'Hey': 28,\n",
              " 'So': 29,\n",
              " 'or': 30,\n",
              " 'time': 31,\n",
              " 'will': 32,\n",
              " 'wan': 33,\n",
              " 'lor...': 34,\n",
              " 'r': 35,\n",
              " \"I'm\": 36,\n",
              " 'we': 37,\n",
              " 'it': 38,\n",
              " 'meet': 39,\n",
              " 'have': 40,\n",
              " 'be': 41,\n",
              " 'dun': 42,\n",
              " 'all': 43,\n",
              " 'no': 44,\n",
              " 'Ü': 45,\n",
              " 'and': 46,\n",
              " 'if': 47,\n",
              " 'still': 48,\n",
              " 'then': 49,\n",
              " 'one': 50,\n",
              " 'do': 51,\n",
              " 'get': 52,\n",
              " 'are': 53,\n",
              " 'going': 54,\n",
              " 'like': 55,\n",
              " 'out': 56,\n",
              " 'come': 57,\n",
              " 'but': 58,\n",
              " 'up': 59,\n",
              " 'But': 60,\n",
              " 'your': 61,\n",
              " 'Hi': 62,\n",
              " 'b': 63,\n",
              " 'ask': 64,\n",
              " 'how': 65,\n",
              " 'wat': 66,\n",
              " 'goin': 67,\n",
              " \"i'm\": 68,\n",
              " 'den': 69,\n",
              " 'when': 70,\n",
              " 'think': 71,\n",
              " 'call': 72,\n",
              " 'w': 73,\n",
              " 'Haha...': 74,\n",
              " 'with': 75,\n",
              " 'home': 76,\n",
              " 'now': 77,\n",
              " 'free': 78,\n",
              " 'back': 79,\n",
              " 'that': 80,\n",
              " 'see': 81,\n",
              " 'buy': 82,\n",
              " 'la...': 83,\n",
              " 'she': 84,\n",
              " 'How': 85,\n",
              " 'juz': 86,\n",
              " 'her': 87,\n",
              " 'abt': 88,\n",
              " 'need': 89,\n",
              " 'just': 90,\n",
              " 'Ok': 91,\n",
              " 'more': 92,\n",
              " 'they': 93,\n",
              " 'where': 94,\n",
              " 'there': 95,\n",
              " 'Then': 96,\n",
              " 'take': 97,\n",
              " 'wanna': 98,\n",
              " 'this': 99,\n",
              " 'was': 100,\n",
              " \"i'll\": 101,\n",
              " 'No': 102,\n",
              " 'We': 103,\n",
              " 'Can': 104,\n",
              " 'Wat': 105,\n",
              " 'liao...': 106,\n",
              " 'say': 107,\n",
              " 'later': 108,\n",
              " 'tt': 109,\n",
              " 'Den': 110,\n",
              " 'from': 111,\n",
              " 'oredi...': 112,\n",
              " 'ah...': 113,\n",
              " ':)': 114,\n",
              " 'Haha': 115,\n",
              " 'If': 116,\n",
              " 'after': 117,\n",
              " 'too': 118,\n",
              " 'da': 119,\n",
              " 'only': 120,\n",
              " '?': 121,\n",
              " 'ard': 122,\n",
              " 'know': 123,\n",
              " 'bring': 124,\n",
              " 'day': 125,\n",
              " 'My': 126,\n",
              " 'make': 127,\n",
              " 'cos': 128,\n",
              " 'leh...': 129,\n",
              " 'oso': 130,\n",
              " 'c': 131,\n",
              " 'noe': 132,\n",
              " 'im': 133,\n",
              " 'find': 134,\n",
              " 'want': 135,\n",
              " 'Oh': 136,\n",
              " 'quite': 137,\n",
              " 'late': 138,\n",
              " \"I'll\": 139,\n",
              " 'tmr': 140,\n",
              " 'u?': 141,\n",
              " 'msg': 142,\n",
              " 'wif': 143,\n",
              " 'intro': 144,\n",
              " 'thk': 145,\n",
              " 'some': 146,\n",
              " 'Hey...': 147,\n",
              " 'Haha,': 148,\n",
              " 'really': 149,\n",
              " 'by': 150,\n",
              " 'You': 151,\n",
              " 'cant': 152,\n",
              " 'ñ': 153,\n",
              " 'liao': 154,\n",
              " 'R': 155,\n",
              " 'Not': 156,\n",
              " 'reach': 157,\n",
              " 'look': 158,\n",
              " 'Where': 159,\n",
              " 'good': 160,\n",
              " 'long': 161,\n",
              " 'ok': 162,\n",
              " 'dunno': 163,\n",
              " 'jus': 164,\n",
              " 'next': 165,\n",
              " 'now?': 166,\n",
              " '...': 167,\n",
              " 'its': 168,\n",
              " 'wait': 169,\n",
              " 'v': 170,\n",
              " 'help': 171,\n",
              " 'ya': 172,\n",
              " 'also': 173,\n",
              " 'new': 174,\n",
              " 'did': 175,\n",
              " 'doing': 176,\n",
              " 'tis': 177,\n",
              " 'Me': 178,\n",
              " 'lunch': 179,\n",
              " 'fren': 180,\n",
              " 'lor.': 181,\n",
              " 'what': 182,\n",
              " 'now...': 183,\n",
              " 'wat...': 184,\n",
              " 'frm': 185,\n",
              " 'care': 186,\n",
              " 'Eh': 187,\n",
              " 'any': 188,\n",
              " 'dinner': 189,\n",
              " 'yr': 190,\n",
              " 'eat': 191,\n",
              " 'am': 192,\n",
              " 'watch': 193,\n",
              " 'sch': 194,\n",
              " 'much': 195,\n",
              " 'Im': 196,\n",
              " 'A': 197,\n",
              " 'hav': 198,\n",
              " 'chat': 199,\n",
              " 'tell': 200,\n",
              " 'Hmmm...': 201,\n",
              " 'he': 202,\n",
              " 'k.': 203,\n",
              " \"it's\": 204,\n",
              " 'change': 205,\n",
              " 'Call': 206,\n",
              " 'tink': 207,\n",
              " 'as': 208,\n",
              " 'first': 209,\n",
              " 'enjoy': 210,\n",
              " 'ah?': 211,\n",
              " 'early': 212,\n",
              " 'nd': 213,\n",
              " 'very': 214,\n",
              " '1': 215,\n",
              " 'coming': 216,\n",
              " 'y': 217,\n",
              " 'nt': 218,\n",
              " 'already': 219,\n",
              " 'Got': 220,\n",
              " 'lor': 221,\n",
              " 'lei...': 222,\n",
              " 'din': 223,\n",
              " 'outside': 224,\n",
              " 'our': 225,\n",
              " 'Ya': 226,\n",
              " 'dont': 227,\n",
              " 'bus': 228,\n",
              " 'Hope': 229,\n",
              " 'had': 230,\n",
              " 'nice': 231,\n",
              " 'gd': 232,\n",
              " 'cut': 233,\n",
              " 'reply': 234,\n",
              " 'Dun': 235,\n",
              " 'today': 236,\n",
              " 'stay': 237,\n",
              " 'u...': 238,\n",
              " 'must': 239,\n",
              " 'feel': 240,\n",
              " 'hope': 241,\n",
              " 'hey': 242,\n",
              " 'sms': 243,\n",
              " 'here': 244,\n",
              " 'Yup...': 245,\n",
              " 'end': 246,\n",
              " 'ür': 247,\n",
              " 'Okay...': 248,\n",
              " 'wana': 249,\n",
              " 'dat': 250,\n",
              " 'sis': 251,\n",
              " 'haf': 252,\n",
              " 'way': 253,\n",
              " 'show': 254,\n",
              " 'been': 255,\n",
              " 'He': 256,\n",
              " 'mind': 257,\n",
              " \"It's\": 258,\n",
              " 'Y': 259,\n",
              " 'now.': 260,\n",
              " 'last': 261,\n",
              " '3': 262,\n",
              " '!': 263,\n",
              " 'Tmr': 264,\n",
              " 'Oh...': 265,\n",
              " 'rite...': 266,\n",
              " 'book': 267,\n",
              " 'lesson': 268,\n",
              " 'sure': 269,\n",
              " 'us': 270,\n",
              " 'saw': 271,\n",
              " 'At': 272,\n",
              " 'Dunno': 273,\n",
              " 'off': 274,\n",
              " 'hp': 275,\n",
              " 'work': 276,\n",
              " 'too...': 277,\n",
              " 'Or': 278,\n",
              " 'finish': 279,\n",
              " 'Cos': 280,\n",
              " 'lah...': 281,\n",
              " 'Ur': 282,\n",
              " 'having': 283,\n",
              " 'working': 284,\n",
              " '12': 285,\n",
              " 'hi': 286,\n",
              " 'didnt': 287,\n",
              " 'place': 288,\n",
              " 'down': 289,\n",
              " 'Good': 290,\n",
              " 'Thk': 291,\n",
              " 'la,': 292,\n",
              " 'again': 293,\n",
              " 'few': 294,\n",
              " 'Take': 295,\n",
              " 'an': 296,\n",
              " 'stuff': 297,\n",
              " 'went': 298,\n",
              " 'bad': 299,\n",
              " 'me...': 300,\n",
              " 'mi': 301,\n",
              " 'ppl': 302,\n",
              " 'muz': 303,\n",
              " 'lect': 304,\n",
              " 'She': 305,\n",
              " 'than': 306,\n",
              " 'right?': 307,\n",
              " 'it?': 308,\n",
              " 'ok.': 309,\n",
              " 'him': 310,\n",
              " 'wont': 311,\n",
              " 'tmr?': 312,\n",
              " 'not?': 313,\n",
              " 'THE': 314,\n",
              " 'Huh...': 315,\n",
              " 'put': 316,\n",
              " 'let': 317,\n",
              " 'coz': 318,\n",
              " '.': 319,\n",
              " 'nite': 320,\n",
              " 'What': 321,\n",
              " 'mean': 322,\n",
              " 'Hee...': 323,\n",
              " 'class': 324,\n",
              " 'lar...': 325,\n",
              " 'rite?': 326,\n",
              " 'ME': 327,\n",
              " 'Okie': 328,\n",
              " 'today?': 329,\n",
              " 'today...': 330,\n",
              " 'who': 331,\n",
              " 'hair': 332,\n",
              " 'yet?': 333,\n",
              " 'b4': 334,\n",
              " 'Hee': 335,\n",
              " 'cya': 336,\n",
              " 'time...': 337,\n",
              " 'try': 338,\n",
              " 'time?': 339,\n",
              " 'other': 340,\n",
              " 'big': 341,\n",
              " 'orchard': 342,\n",
              " 'havent': 343,\n",
              " 'é': 344,\n",
              " 'haven': 345,\n",
              " 'someone': 346,\n",
              " 'number': 347,\n",
              " 'pass': 348,\n",
              " 'send': 349,\n",
              " 'give': 350,\n",
              " 'ah,': 351,\n",
              " 'sat': 352,\n",
              " 'nus': 353,\n",
              " 'sweet': 354,\n",
              " 'Haha.': 355,\n",
              " 'meeting': 356,\n",
              " 'always': 357,\n",
              " 'one...': 358,\n",
              " 'gonna': 359,\n",
              " 'm': 360,\n",
              " 'use': 361,\n",
              " 'pay': 362,\n",
              " 'Help': 363,\n",
              " 'Go': 364,\n",
              " 'play': 365,\n",
              " 'another': 366,\n",
              " 'lk': 367,\n",
              " 'drivin': 368,\n",
              " 'oso...': 369,\n",
              " 'shop': 370,\n",
              " 'confirm': 371,\n",
              " 'anything': 372,\n",
              " 'before': 373,\n",
              " 'short': 374,\n",
              " 'frens': 375,\n",
              " 'study': 376,\n",
              " 'hse': 377,\n",
              " 'sorry': 378,\n",
              " 'Yupz...': 379,\n",
              " 'around': 380,\n",
              " 'la..': 381,\n",
              " 'Is': 382,\n",
              " 'lot': 383,\n",
              " 'Oh,': 384,\n",
              " 'mon': 385,\n",
              " 'things': 386,\n",
              " 'there?': 387,\n",
              " 'near': 388,\n",
              " 'those': 389,\n",
              " 'Okie...': 390,\n",
              " 'lah.': 391,\n",
              " 'Did': 392,\n",
              " 'oredi': 393,\n",
              " 'mrt': 394,\n",
              " 'pls': 395,\n",
              " 'driving': 396,\n",
              " 'maybe': 397,\n",
              " 'And': 398,\n",
              " 'Lea': 399,\n",
              " 'days': 400,\n",
              " 'workin': 401,\n",
              " 'done': 402,\n",
              " 'Juz': 403,\n",
              " 'gal': 404,\n",
              " 'mayb': 405,\n",
              " 'Sorry': 406,\n",
              " 'lor,': 407,\n",
              " 'When': 408,\n",
              " '..': 409,\n",
              " 'They': 410,\n",
              " 'hard': 411,\n",
              " 'great': 412,\n",
              " 'dinner...': 413,\n",
              " 'wk': 414,\n",
              " 'told': 415,\n",
              " 'Ask': 416,\n",
              " '&': 417,\n",
              " 'Dont': 418,\n",
              " 'fun': 419,\n",
              " 'wake': 420,\n",
              " 'until': 421,\n",
              " 'wun': 422,\n",
              " 'love': 423,\n",
              " 'money': 424,\n",
              " 'said': 425,\n",
              " 'Haven': 426,\n",
              " 'over': 427,\n",
              " 'which': 428,\n",
              " 'keep': 429,\n",
              " 'phone': 430,\n",
              " 'IM': 431,\n",
              " 'house': 432,\n",
              " 'can?': 433,\n",
              " 'Have': 434,\n",
              " 'well': 435,\n",
              " 'many': 436,\n",
              " 'Just': 437,\n",
              " 'cannot': 438,\n",
              " 'name': 439,\n",
              " 'havin': 440,\n",
              " 'guys': 441,\n",
              " 'huh...': 442,\n",
              " 'it...': 443,\n",
              " 'Cya': 444,\n",
              " 'ok...': 445,\n",
              " \"don't\": 446,\n",
              " 'shld': 447,\n",
              " 'bought': 448,\n",
              " 'Joey:': 449,\n",
              " 'forget': 450,\n",
              " 'bit': 451,\n",
              " 'into': 452,\n",
              " 'hall': 453,\n",
              " 'rest': 454,\n",
              " 'The': 455,\n",
              " '6': 456,\n",
              " 'Eh...': 457,\n",
              " 'Really': 458,\n",
              " 'me?': 459,\n",
              " 'check': 460,\n",
              " 'sleep': 461,\n",
              " 'same': 462,\n",
              " 'IS': 463,\n",
              " 'doin': 464,\n",
              " 'far': 465,\n",
              " 'tat': 466,\n",
              " 'about': 467,\n",
              " 'dear': 468,\n",
              " 'leave': 469,\n",
              " 'not...': 470,\n",
              " 'shall': 471,\n",
              " 'le': 472,\n",
              " 'HI': 473,\n",
              " 'Mayb': 474,\n",
              " 'happy': 475,\n",
              " 'thing': 476,\n",
              " 'me.': 477,\n",
              " 'town': 478,\n",
              " 'leh,': 479,\n",
              " 'job': 480,\n",
              " 'slp': 481,\n",
              " 'k': 482,\n",
              " 'online': 483,\n",
              " 'lotsa': 484,\n",
              " 'blue': 485,\n",
              " 'male': 486,\n",
              " 'go?': 487,\n",
              " 'E': 488,\n",
              " 'MY': 489,\n",
              " 'might': 490,\n",
              " 'pick': 491,\n",
              " 'neva': 492,\n",
              " 'soon': 493,\n",
              " 'leh.': 494,\n",
              " 'first.': 495,\n",
              " 'birthday': 496,\n",
              " 'better': 497,\n",
              " 'Cant': 498,\n",
              " 'join': 499,\n",
              " 'u.': 500,\n",
              " 'Maybe': 501,\n",
              " 'Anyway,': 502,\n",
              " 'worry': 503,\n",
              " 'Of': 504,\n",
              " 'bk': 505,\n",
              " 'Hey,': 506,\n",
              " 'photo': 507,\n",
              " 'Do': 508,\n",
              " 'notes': 509,\n",
              " 'bout': 510,\n",
              " 'nothing': 511,\n",
              " 'tmr...': 512,\n",
              " 'left': 513,\n",
              " 'wil': 514,\n",
              " 'dog': 515,\n",
              " '1st': 516,\n",
              " 'leh': 517,\n",
              " 'how?': 518,\n",
              " 'already.': 519,\n",
              " 'Still': 520,\n",
              " 'lah,': 521,\n",
              " 'camp': 522,\n",
              " 'sci': 523,\n",
              " 'life': 524,\n",
              " 'doing?': 525,\n",
              " 'else': 526,\n",
              " 'his': 527,\n",
              " 'friends': 528,\n",
              " 'thn': 529,\n",
              " 'too.': 530,\n",
              " 'means': 531,\n",
              " 'something': 532,\n",
              " 'ah.': 533,\n",
              " 'Huh': 534,\n",
              " 'later...': 535,\n",
              " 'now..': 536,\n",
              " 'Wanna': 537,\n",
              " 'Happy': 538,\n",
              " 'wish': 539,\n",
              " 'shopping': 540,\n",
              " 'afternoon': 541,\n",
              " 'parents': 542,\n",
              " '=)': 543,\n",
              " 'Anyway': 544,\n",
              " 'leona': 545,\n",
              " 'K': 546,\n",
              " 'soon.': 547,\n",
              " 'both': 548,\n",
              " 'fri': 549,\n",
              " 'tml': 550,\n",
              " 'already...': 551,\n",
              " 'tv': 552,\n",
              " 'busy': 553,\n",
              " 'Yun': 554,\n",
              " 'Yup.': 555,\n",
              " 'came': 556,\n",
              " 'able': 557,\n",
              " 'email': 558,\n",
              " 'Today': 559,\n",
              " 'tired': 560,\n",
              " 'part': 561,\n",
              " 'them': 562,\n",
              " 'IN': 563,\n",
              " 'Hi!': 564,\n",
              " 'called': 565,\n",
              " 'dad': 566,\n",
              " 'save': 567,\n",
              " 'seat': 568,\n",
              " 'On': 569,\n",
              " 'chat?': 570,\n",
              " 'comin': 571,\n",
              " 'com': 572,\n",
              " 'pple': 573,\n",
              " 'time.': 574,\n",
              " 'Later': 575,\n",
              " 'Now': 576,\n",
              " \"there's\": 577,\n",
              " 'bbq': 578,\n",
              " 'lah': 579,\n",
              " 'Elaine': 580,\n",
              " 'Gee...': 581,\n",
              " 'till': 582,\n",
              " 'xin': 583,\n",
              " 'start': 584,\n",
              " 'nvr': 585,\n",
              " 'exams': 586,\n",
              " 'It': 587,\n",
              " 'bugis': 588,\n",
              " 'la': 589,\n",
              " 'going?': 590,\n",
              " 'interested': 591,\n",
              " 'haha..': 592,\n",
              " 'nvm': 593,\n",
              " 'k?': 594,\n",
              " 'forgot': 595,\n",
              " 'Yup': 596,\n",
              " 'gotta': 597,\n",
              " 'All': 598,\n",
              " 'movie': 599,\n",
              " 'nxt': 600,\n",
              " 'tot': 601,\n",
              " 'fine': 602,\n",
              " 'yet.': 603,\n",
              " 'ù': 604,\n",
              " 'week': 605,\n",
              " 'dreams': 606,\n",
              " 'Will': 607,\n",
              " 'tai': 608,\n",
              " 'were': 609,\n",
              " 'may': 610,\n",
              " 'late...': 611,\n",
              " 'right': 612,\n",
              " 'Okay.': 613,\n",
              " 'ah': 614,\n",
              " 'Hello': 615,\n",
              " 'test': 616,\n",
              " 'Hey!': 617,\n",
              " 'actually': 618,\n",
              " 'Same': 619,\n",
              " 'chinese': 620,\n",
              " 'year': 621,\n",
              " 'mine': 622,\n",
              " 'drinks': 623,\n",
              " 'kind': 624,\n",
              " 'haha': 625,\n",
              " \"That's\": 626,\n",
              " \"i've\": 627,\n",
              " \"can't\": 628,\n",
              " 'talk': 629,\n",
              " 'fetch': 630,\n",
              " 'sad': 631,\n",
              " 'wrong': 632,\n",
              " 'ma...': 633,\n",
              " 'not.': 634,\n",
              " 'today.': 635,\n",
              " 'watching': 636,\n",
              " 'sit': 637,\n",
              " 'HEY': 638,\n",
              " 'U?': 639,\n",
              " 'smth': 640,\n",
              " \"Wat's\": 641,\n",
              " \"wat's\": 642,\n",
              " 'disturb': 643,\n",
              " 'lor..': 644,\n",
              " 'Hmmm....': 645,\n",
              " 'abit': 646,\n",
              " 'guess': 647,\n",
              " 'now,': 648,\n",
              " \"How's\": 649,\n",
              " 'ma': 650,\n",
              " 'Are': 651,\n",
              " 'yun': 652,\n",
              " 'cum': 653,\n",
              " 'Mine': 654,\n",
              " 'damn': 655,\n",
              " 'Its': 656,\n",
              " 'found': 657,\n",
              " '5': 658,\n",
              " 'again...': 659,\n",
              " 'open': 660,\n",
              " 'school': 661,\n",
              " 'choose': 662,\n",
              " 'coffee': 663,\n",
              " 'Kaiez...': 664,\n",
              " '=5': 665,\n",
              " 'collect': 666,\n",
              " 'leh?': 667,\n",
              " 'Study': 668,\n",
              " 'miss': 669,\n",
              " 't': 670,\n",
              " 'old': 671,\n",
              " 'female': 672,\n",
              " 'out...': 673,\n",
              " 'li': 674,\n",
              " 'ok?': 675,\n",
              " 'never': 676,\n",
              " 'd': 677,\n",
              " 'TO': 678,\n",
              " 'dance': 679,\n",
              " 'eng': 680,\n",
              " 'veri': 681,\n",
              " 'hows': 682,\n",
              " 'thought': 683,\n",
              " 'cheese': 684,\n",
              " 'you.': 685,\n",
              " 'school?': 686,\n",
              " 'liao.': 687,\n",
              " 'UR': 688,\n",
              " 'May': 689,\n",
              " \"how's\": 690,\n",
              " 'bishan': 691,\n",
              " 'exam': 692,\n",
              " 'real': 693,\n",
              " 'Pls': 694,\n",
              " 'UP': 695,\n",
              " 'ans': 696,\n",
              " 'Okay': 697,\n",
              " \"I've\": 698,\n",
              " 'yet': 699,\n",
              " ',': 700,\n",
              " 'present': 701,\n",
              " 'tonight': 702,\n",
              " 'tut': 703,\n",
              " 'alone': 704,\n",
              " 'bot': 705,\n",
              " 'oredi?': 706,\n",
              " 'darlin': 707,\n",
              " 'sign': 708,\n",
              " 'studyin': 709,\n",
              " 'without': 710,\n",
              " 'go...': 711,\n",
              " 'pls?': 712,\n",
              " 'FOR': 713,\n",
              " 'Lk': 714,\n",
              " 'anot?': 715,\n",
              " 'food': 716,\n",
              " 'thurs': 717,\n",
              " 'Thanx': 718,\n",
              " 'Very': 719,\n",
              " 'sun': 720,\n",
              " 'decide': 721,\n",
              " 'celebrate': 722,\n",
              " 'close': 723,\n",
              " 'break': 724,\n",
              " 'Hmm...': 725,\n",
              " 'interested?': 726,\n",
              " 'trip': 727,\n",
              " 'duno': 728,\n",
              " 'should': 729,\n",
              " 'Nope': 730,\n",
              " 'TIME': 731,\n",
              " \"he's\": 732,\n",
              " 'gettin': 733,\n",
              " 'thanks': 734,\n",
              " 'u..': 735,\n",
              " 'feeling': 736,\n",
              " 'WAS': 737,\n",
              " 'bt': 738,\n",
              " 'den...': 739,\n",
              " 'sunday': 740,\n",
              " 'night': 741,\n",
              " 'comp': 742,\n",
              " \"u'll\": 743,\n",
              " 'huh?': 744,\n",
              " 'case': 745,\n",
              " 'C': 746,\n",
              " 'late.': 747,\n",
              " 'Try': 748,\n",
              " 'diff': 749,\n",
              " 'de': 750,\n",
              " 'OR': 751,\n",
              " 'NOT': 752,\n",
              " 'lecture': 753,\n",
              " 'price': 754,\n",
              " 'watchin': 755,\n",
              " 'sell': 756,\n",
              " 'yet...': 757,\n",
              " 'lesson...': 758,\n",
              " \"who's\": 759,\n",
              " 'together': 760,\n",
              " 'Nope...': 761,\n",
              " 'took': 762,\n",
              " 'aft': 763,\n",
              " 'proj': 764,\n",
              " 'seats': 765,\n",
              " 'shuhui': 766,\n",
              " 'fr': 767,\n",
              " 'fone': 768,\n",
              " 'discuss': 769,\n",
              " 'reached': 770,\n",
              " 'prob': 771,\n",
              " 'up.': 772,\n",
              " 'Really...': 773,\n",
              " 'enuff': 774,\n",
              " 'Ya...': 775,\n",
              " 'èn': 776,\n",
              " 'n.n': 777,\n",
              " 'would': 778,\n",
              " 'studying': 779,\n",
              " 'mei': 780,\n",
              " 'morning': 781,\n",
              " 'uni': 782,\n",
              " 'Ok...': 783,\n",
              " 'Coz': 784,\n",
              " 'then...': 785,\n",
              " '25': 786,\n",
              " 'LOVE': 787,\n",
              " 'win': 788,\n",
              " 'along': 789,\n",
              " 'instead': 790,\n",
              " 'changed': 791,\n",
              " 'Get': 792,\n",
              " 'day...': 793,\n",
              " 'Huh?': 794,\n",
              " 'receive': 795,\n",
              " '10': 796,\n",
              " 'out.': 797,\n",
              " 'here...': 798,\n",
              " 'Which': 799,\n",
              " 'colour': 800,\n",
              " 'hv': 801,\n",
              " 'o': 802,\n",
              " 'also.': 803,\n",
              " 'bf': 804,\n",
              " 'add': 805,\n",
              " 'walk': 806,\n",
              " 'dunno...': 807,\n",
              " '7': 808,\n",
              " 'past': 809,\n",
              " 'pink': 810,\n",
              " \"He's\": 811,\n",
              " 'tonight?': 812,\n",
              " 'learn': 813,\n",
              " 'Why': 814,\n",
              " 'taxi': 815,\n",
              " 'even': 816,\n",
              " 'In': 817,\n",
              " 'table': 818,\n",
              " 'u!': 819,\n",
              " 'lots': 820,\n",
              " 'la.': 821,\n",
              " 'Ay': 822,\n",
              " 'thru': 823,\n",
              " 'N': 824,\n",
              " 'anyway': 825,\n",
              " '8': 826,\n",
              " 'has': 827,\n",
              " 'izzit?': 828,\n",
              " 'stil': 829,\n",
              " 'cute': 830,\n",
              " 'then.': 831,\n",
              " 'Enjoy': 832,\n",
              " 'Anyone': 833,\n",
              " 'it,': 834,\n",
              " 'later?': 835,\n",
              " 'Hehe..': 836,\n",
              " 'wat?': 837,\n",
              " 'laptop': 838,\n",
              " 'too?': 839,\n",
              " 'beta': 840,\n",
              " 'Hi,': 841,\n",
              " 'handphone': 842,\n",
              " 'centre': 843,\n",
              " 'remember': 844,\n",
              " 'been?': 845,\n",
              " 'yourself': 846,\n",
              " 'Cannot': 847,\n",
              " 'fun...': 848,\n",
              " 'finished': 849,\n",
              " 'Kate': 850,\n",
              " 'IVE': 851,\n",
              " 'AND': 852,\n",
              " 'first...': 853,\n",
              " 'hw': 854,\n",
              " 'thgt': 855,\n",
              " 'two': 856,\n",
              " 'chose': 857,\n",
              " 'line': 858,\n",
              " 'enough': 859,\n",
              " 'xy': 860,\n",
              " 'dinner.': 861,\n",
              " 'meet?': 862,\n",
              " 'sent': 863,\n",
              " 'jos': 864,\n",
              " 'managed': 865,\n",
              " '1245': 866,\n",
              " 'le..': 867,\n",
              " 'bday': 868,\n",
              " 'stand': 869,\n",
              " 'heart': 870,\n",
              " 'Yah...': 871,\n",
              " 'drive': 872,\n",
              " 'airport': 873,\n",
              " 'used': 874,\n",
              " 'stop': 875,\n",
              " 'shd': 876,\n",
              " 'aust': 877,\n",
              " 'since': 878,\n",
              " 'Haiz': 879,\n",
              " 'bored': 880,\n",
              " 'suntec': 881,\n",
              " 'HOW': 882,\n",
              " 'HOPE': 883,\n",
              " 'NO': 884,\n",
              " '20': 885,\n",
              " 'dinner?': 886,\n",
              " \"we'll\": 887,\n",
              " 'wat..': 888,\n",
              " 'using': 889,\n",
              " 'ON': 890,\n",
              " 'do...': 891,\n",
              " 'wuz': 892,\n",
              " 'mind...': 893,\n",
              " 'Kaiez,': 894,\n",
              " 'you?': 895,\n",
              " 'there...': 896,\n",
              " 'mth': 897,\n",
              " 'bed': 898,\n",
              " 'all...': 899,\n",
              " 'Going': 900,\n",
              " 'CARE': 901,\n",
              " 'WHAT': 902,\n",
              " 'luck': 903,\n",
              " 's': 904,\n",
              " 'mah...': 905,\n",
              " 'could': 906,\n",
              " 'so...': 907,\n",
              " 'anytime': 908,\n",
              " 'ger': 909,\n",
              " 'Thought': 910,\n",
              " 'Whats': 911,\n",
              " 'guy': 912,\n",
              " 'slightly': 913,\n",
              " 'early...': 914,\n",
              " 'Yar': 915,\n",
              " 'acct': 916,\n",
              " 'news': 917,\n",
              " 'min': 918,\n",
              " 'evening': 919,\n",
              " 'thur': 920,\n",
              " 'back?': 921,\n",
              " 'This': 922,\n",
              " 'room...': 923,\n",
              " 'meh?': 924,\n",
              " 'ar...': 925,\n",
              " 'Ok,': 926,\n",
              " 'canteen': 927,\n",
              " 'sun?': 928,\n",
              " 'hand': 929,\n",
              " 'lah..': 930,\n",
              " 'you...': 931,\n",
              " 'k...': 932,\n",
              " 'ah!': 933,\n",
              " 'Thgt': 934,\n",
              " 'exercise': 935,\n",
              " 'less': 936,\n",
              " 'sick': 937,\n",
              " 'everyone': 938,\n",
              " 'catch': 939,\n",
              " 'kiss': 940,\n",
              " 'luv': 941,\n",
              " 'daddy': 942,\n",
              " 'me,': 943,\n",
              " 'little': 944,\n",
              " 'tel': 945,\n",
              " 'Wah': 946,\n",
              " 'Nvr': 947,\n",
              " 'eh': 948,\n",
              " 'haha,': 949,\n",
              " 'Aiya,': 950,\n",
              " 'See': 951,\n",
              " 'woke': 952,\n",
              " 'treat': 953,\n",
              " 'wad': 954,\n",
              " 'talking': 955,\n",
              " 'Feel': 956,\n",
              " 'liao..': 957,\n",
              " 'almost': 958,\n",
              " 'nv': 959,\n",
              " 'nt?': 960,\n",
              " 'cake': 961,\n",
              " 'asking': 962,\n",
              " 'WERE': 963,\n",
              " 'please': 964,\n",
              " 'Shld': 965,\n",
              " 'whole': 966,\n",
              " 'being': 967,\n",
              " 'DO': 968,\n",
              " 'shit': 969,\n",
              " 'Muz': 970,\n",
              " 'anything...': 971,\n",
              " 'Was': 972,\n",
              " 'AT': 973,\n",
              " 'reaching': 974,\n",
              " 'thats': 975,\n",
              " 'ya...': 976,\n",
              " 'msg?': 977,\n",
              " 'lovely': 978,\n",
              " 'run': 979,\n",
              " 'body': 980,\n",
              " 'pretty': 981,\n",
              " 'because': 982,\n",
              " 'hour': 983,\n",
              " 'turn': 984,\n",
              " 'no.': 985,\n",
              " 'most': 986,\n",
              " 'matter': 987,\n",
              " 'sports': 988,\n",
              " 'car': 989,\n",
              " 'ex': 990,\n",
              " 'normal': 991,\n",
              " 'Wah.': 992,\n",
              " 'ARE': 993,\n",
              " 'one..': 994,\n",
              " 'can.': 995,\n",
              " 'can,': 996,\n",
              " 'Shall': 997,\n",
              " 'course': 998,\n",
              " 'camp.': 999,\n",
              " 'office': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvGXhLtgsBGx"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tex1A1d6mqIW"
      },
      "source": [
        "# class Dataset:\n",
        "#     def __init__(self, data, tknizer_ita, tknizer_eng, max_len):\n",
        "#         self.encoder_inps = data['italian'].values\n",
        "#         self.decoder_inps = data['d_english_input'].values\n",
        "#         self.decoder_outs = data['d_english_output'].values\n",
        "#         self.tknizer_eng = tknizer_eng\n",
        "#         self.tknizer_ita = tknizer_ita\n",
        "#         self.max_len = max_len\n",
        "\n",
        "#     def __getitem__(self, i):\n",
        "#         self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "#         self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
        "#         self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "#         self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "#         self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "#         self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "#         return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "#     def __len__(self): \n",
        "#         return len(self.encoder_inps)\n",
        "\n",
        "    \n",
        "# class Dataloder(tf.keras.utils.Sequence):    \n",
        "#     def __init__(self, dataset, batch_size=1):\n",
        "#         self.dataset = dataset\n",
        "#         self.batch_size = batch_size\n",
        "#         self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "\n",
        "#     def __getitem__(self, i):\n",
        "#         start = i * self.batch_size\n",
        "#         stop = (i + 1) * self.batch_size\n",
        "#         data = []\n",
        "#         for j in range(start, stop):\n",
        "#             data.append(self.dataset[j])\n",
        "\n",
        "#         batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "#         # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "#         return tuple([[batch[0],batch[1]],batch[2]])\n",
        "\n",
        "#     def __len__(self):  # your model.fit_gen requires this function\n",
        "#         return len(self.indexes) // self.batch_size\n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8ywIUJ0NW1I"
      },
      "source": [
        "# class Dataset:\n",
        "#     def __init__(self, data, tknizer_ita, tknizer_eng, max_len):\n",
        "#         self.encoder_inps = data['SMS_TEXT'].values\n",
        "#         self.decoder_inps = data['ENGLISH_INPUT'].values\n",
        "#         self.decoder_outs = data['ENGLISH_OUTPUT'].values\n",
        "#         self.tknizer_eng = tknizer_eng\n",
        "#         self.tknizer_ita = tknizer_ita\n",
        "#         self.max_len = max_len\n",
        "#         # self.max_len1=max_len1\n",
        " \n",
        "#     def __getitem__(self, i):\n",
        "#         self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "#         self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
        "#         self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
        " \n",
        "#         self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "#         self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "#         self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "#         return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        " \n",
        "#     def __len__(self): # your model.fit_gen requires this function\n",
        "#         return len(self.encoder_inps)\n",
        " \n",
        "    \n",
        "# class Dataloder(tf.keras.utils.Sequence):    \n",
        "#     def __init__(self, dataset, batch_size=1):\n",
        "#         self.dataset = dataset\n",
        "#         self.batch_size = batch_size\n",
        "#         self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        " \n",
        " \n",
        "#     def __getitem__(self, i):\n",
        "#         start = i * self.batch_size\n",
        "#         stop = (i + 1) * self.batch_size\n",
        "#         data = []\n",
        "#         for j in range(start, stop):\n",
        "#             data.append(self.dataset[j])\n",
        " \n",
        "#         batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "#         # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "#         return tuple([[batch[0],batch[1]],batch[2]])\n",
        " \n",
        "#     def __len__(self):  # your model.fit_gen requires this function\n",
        "#         return len(self.indexes) // self.batch_size\n",
        " \n",
        "#     def on_epoch_end(self):\n",
        "#         self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdjHPdPxJavM"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_5NA24KzRp"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        \n",
        "      super().__init__()\n",
        "\n",
        "      self.e_embed = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length= input_length)\n",
        "      self.lstm_size = lstm_size\n",
        "      \n",
        "      self.e_lstm = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- encoder_output, last time step's hidden and cell state\n",
        "        '''\n",
        "        embedding = self.e_embed(input_sequence)\n",
        "        # print(embedding.shape)\n",
        "        # output_state, enc_h, enc_c = self.e_lstm( embedding, initial_state = states)\n",
        "        output_state, enc_h, enc_c = self.e_lstm( embedding)\n",
        "\n",
        "        return output_state, enc_h, enc_c\n",
        "\n",
        "      \n",
        "\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXn278lhLYRM"
      },
      "source": [
        "<font color='blue'>**Attention**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab5SNdPZLlur"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.scoring_function = scoring_function\n",
        "\n",
        "\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "\n",
        "    if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      self.dot= tf.keras.layers.Dot(axes = (1,2))      \n",
        "      pass\n",
        "    elif scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "      self.w= Dense(att_units)\n",
        "      self.dot = tf.keras.layers.Dot(axes = (1,2))\n",
        "      pass\n",
        "    elif scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      self.w1= Dense(att_units)\n",
        "      self.w2= Dense(att_units)\n",
        "      self.v= Dense(1)\n",
        "      pass\n",
        "  \n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "    '''\n",
        "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "        Multiply the score function with your encoder_outputs to get the context vector.\n",
        "        Function returns context vector and attention weights(softmax - scores)\n",
        "    '''\n",
        "    decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
        "    if self.scoring_function == 'dot':\n",
        "        # Implement Dot score function here\n",
        "        score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0,2,1)), encoder_output]), (0,2,1))\n",
        "        # print(\"In Dot\")\n",
        "\n",
        "        pass\n",
        "    elif self.scoring_function == 'general':\n",
        "        # Implement General score function here\n",
        "        # print(encoder_output.shape)\n",
        "        mulpy = self.w(encoder_output)\n",
        "        # print(mulpy.shape)\n",
        "        score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mulpy]), (0, 2,1),)\n",
        "\n",
        "        pass\n",
        "    elif self.scoring_function == 'concat':\n",
        "        inte = self.w1(decoder_hidden_state) + self.w2(encoder_output)\n",
        "        tan = tf.nn.tanh(inte)\n",
        "        score = self.v(tan)\n",
        "\n",
        "    att_weights = tf.nn.softmax(score, axis =1)\n",
        "    context_vector = att_weights * encoder_output\n",
        "    context_vector = tf.reduce_sum( context_vector, axis=1)\n",
        "    return context_vector, att_weights\n",
        "\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-FNEbfL2DN"
      },
      "source": [
        "<font color='blue'>**OneStepDecoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8m7lmOL097"
      },
      "source": [
        "class OneStepDecoder(tf.keras.Model):\n",
        "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "\n",
        "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "      super(OneStepDecoder, self).__init__()\n",
        "      self.dec_embed = Embedding(input_dim = tar_vocab_size, output_dim = embedding_dim)\n",
        "      self.lstm = LSTM(dec_units, return_sequences = True, return_state = True)\n",
        "      self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
        "      self.den = Dense(tar_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "    '''\n",
        "        One step decoder mechanisim step by step:\n",
        "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "      C. Concat the context vector with the step A output\n",
        "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "    '''\n",
        "    embd = self.dec_embed(input_to_decoder)\n",
        "    context_vec, attention_weights = self.attention( state_h,  encoder_output)    \n",
        "\n",
        "    f_inp = tf.concat([tf.expand_dims(context_vec, 1), embd], axis = -1)\n",
        "    # print(f_inp.shape)\n",
        "    out, dec_h, dec_c = self.lstm(f_inp, [state_h, state_c])\n",
        "    out = tf.reshape( out, (-1, out.shape[2]))\n",
        "    output = self.den(out)\n",
        "\n",
        "    return output, dec_h, dec_c, attention_weights, context_vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Boej-H0jDtMG"
      },
      "source": [
        "#  model2.layers[1].onestepdecoder(dec_input,encoder_outputs,\n",
        "#                                                                                state_h,state_c,training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHrurjUMGAi"
      },
      "source": [
        "<font color='blue'>**Decoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-x31rj6Hc4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "      super(Decoder, self).__init__()\n",
        "      self.input_length = input_length\n",
        "      self.out_vocab_size = out_vocab_size\n",
        "      self.oneStepDecoder = OneStepDecoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "      self.out_vocab_size = out_vocab_size\n",
        "        \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        \n",
        "        #Iterate till the length of the decoder input\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            # Store the output in tensorarray\n",
        "        # Return the tensor array\n",
        "        # outputs = tf.TensorArray(dtype =  tf.float32, size= input_to_decoder.shape[1])\n",
        "        outputs = tf.TensorArray(dtype =  tf.float32, size= tf.shape(input_to_decoder)[1])\n",
        "\n",
        "        \n",
        "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
        "\n",
        "            output, decoder_hidden_state, decoder_cell_state, _, _ = self.oneStepDecoder(input_to_decoder[:, timestep:timestep+1],\n",
        "                                                                                          encoder_output,decoder_hidden_state,decoder_cell_state)                                                                                            \n",
        "                                                                                             \n",
        "                                                                                             \n",
        "            # Store the output in tensorarray\n",
        "            outputs = outputs.write(timestep, output)\n",
        "        # Return the tensor array\n",
        "        outputs = tf.transpose(outputs.stack(), (1, 0, 2))\n",
        "        return outputs\n",
        "        \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1T1EOoMTqC"
      },
      "source": [
        "<font color='blue'>**Encoder Decoder model**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfqBIe20MT3D"
      },
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "  def __init__(self,inp_vocab_size, out_vocab_size,input_length, enc_units,embedding_dim, dec_units, max_len, score_fun, att_units, batch_size):\n",
        "    #Intialize objects from encoder decoder\n",
        "    super(encoder_decoder, self).__init__()\n",
        "    self.encoder = Encoder(inp_vocab_size= inp_vocab_size +1, embedding_size=embedding_dim, lstm_size= att_units,input_length= max_len)\n",
        "    self.decoder = Decoder(out_vocab_size +1, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  \n",
        "  def call(self,data):\n",
        "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "    # return the decoder output\n",
        "    e_inp, d_inp = data[0], data[1]\n",
        "    # print(data[0].shape)\n",
        "    # print(data[1].shape)\n",
        "\n",
        "\n",
        "    initial_state = self.encoder.initialize_states(self.batch_size)\n",
        "\n",
        "    e_output, enc_h, enc_c = self.encoder(e_inp,initial_state)\n",
        "    outputs = tf.TensorArray(dtype = tf.float32, size= 20)\n",
        "        \n",
        "    dec_h = enc_h\n",
        "    dec_c = enc_c\n",
        "    # print(dec_h.shape)\n",
        "    output=self.decoder(d_inp,e_output, dec_h, dec_c)\n",
        "  \n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRxB-FDMJWL"
      },
      "source": [
        "<font color='blue'>**Custom loss function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY_3izrXMs8y"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
        "\n",
        "def custom_lossfunction(targets,logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  loss_ = loss_object(targets, logits)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "# def custom_lossfunction(targets,logits):\n",
        "\n",
        "  # Custom loss function that will not consider the loss for padded zeros.\n",
        "  # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QlbWAqNNlqe"
      },
      "source": [
        "<font color='blue'>**Training**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqtZUQF2NuZE"
      },
      "source": [
        "Implement dot function here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgyWwZWeMxGQ"
      },
      "source": [
        "# Implement teacher forcing while training your model. You can do it two ways.\n",
        "# Prepare your data, encoder_input,decoder_input and decoder_output\n",
        "# if decoder input is \n",
        "# <start> Hi how are you\n",
        "# decoder output should be\n",
        "# Hi How are you <end>\n",
        "# i.e when you have send <start>-- decoder predicted Hi, 'Hi' decoder predicted 'How' .. e.t.c\n",
        "\n",
        "# or\n",
        " \n",
        "# model.fit([train_ita,train_eng],train_eng[:,1:]..)\n",
        "# Note: If you follow this approach some grader functions might return false and this is fine."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNRsc0GPuaUC"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, tknizer_ita, tknizer_eng, max_len,max_len1):\n",
        "        self.encoder_inps = data['SMS_TEXT'].values\n",
        "        self.decoder_inps = data['ENGLISH_INPUT'].values\n",
        "        self.decoder_outs = data['ENGLISH_OUTPUT'].values\n",
        "        self.tknizer_eng = tknizer_eng\n",
        "        self.tknizer_ita = tknizer_ita\n",
        "        self.max_len = max_len\n",
        "        self.max_len1=max_len1\n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
        " \n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len1, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len1, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        " \n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        " \n",
        "    \n",
        "class Dataloder(tf.keras.utils.Sequence):    \n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        " \n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        " \n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "        return tuple([[batch[0],batch[1]],batch[2]])\n",
        " \n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        " \n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtYqvcqvufUH"
      },
      "source": [
        "train_dataset = Dataset(train_data, tokenizer, tokenizer_e, 39,40)\n",
        "test_dataset  = Dataset(test_data, tokenizer, tokenizer_e, 39,40)\n",
        " \n",
        "train_dataloader = Dataloder(train_dataset, batch_size=64)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Za9GYu6u9SS"
      },
      "source": [
        "# model = encoder_decoder(len(encoder_vocabluary), len(decoder_vocabluary),input_length, lstm_size, embedding_dim, att_units, max_len, 'dot', att_units, batch_size)\n",
        "# model.compile(optimizer = 'Adam', loss = custom_lossfunction)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw4jhKeI8FDI"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "    pred_value= K.cast(K.argmax(y_pred, axis=-1), dtype='float32')\n",
        "    true_value = K.cast(K.equal(y_true, pred_value), dtype='float32')\n",
        "\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * true_value)\n",
        "    n_total = K.sum(mask)\n",
        "  \n",
        "    return n_correct / n_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMyAx1q7vT5e"
      },
      "source": [
        "lstm_size = 100\n",
        "embedding_dim =128\n",
        "att_units = 100\n",
        "dec_units=100\n",
        "batch_size = 64 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thk0nRrjuhV8",
        "outputId": "70681ca0-6db3-4f2d-d171-53184a024052"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model2  = encoder_decoder(encoder_vocab_size,decoder_vocab_size,39,lstm_size,embedding_dim,att_units, 40,'concat',dec_units,batch_size)\n",
        "optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "model2.compile(optimizer=optimizer,loss=custom_lossfunction,metrics=[accuracy])\n",
        "train_steps=train_data.shape[0]//64\n",
        "valid_steps=test_data.shape[0]//20\n",
        "model2.fit(train_dataloader, steps_per_epoch=train_steps, epochs=50, validation_data=test_dataloader, validation_steps=valid_steps)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - 30s 768ms/step - loss: 2.7525 - accuracy: 0.0712 - val_loss: 2.0542 - val_accuracy: 0.0898\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 22s 726ms/step - loss: 2.5893 - accuracy: 0.0767 - val_loss: 2.0270 - val_accuracy: 0.0938\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 22s 716ms/step - loss: 2.5480 - accuracy: 0.0816 - val_loss: 1.9935 - val_accuracy: 0.0977\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 21s 709ms/step - loss: 2.5218 - accuracy: 0.0853 - val_loss: 1.9663 - val_accuracy: 0.1055\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 22s 716ms/step - loss: 2.4275 - accuracy: 0.0935 - val_loss: 1.9318 - val_accuracy: 0.1133\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 22s 719ms/step - loss: 2.2555 - accuracy: 0.1220 - val_loss: 1.8514 - val_accuracy: 0.1172\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 22s 720ms/step - loss: 2.0829 - accuracy: 0.1552 - val_loss: 1.7958 - val_accuracy: 0.1328\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 21s 712ms/step - loss: 1.9481 - accuracy: 0.1738 - val_loss: 1.7720 - val_accuracy: 0.1484\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 21s 711ms/step - loss: 1.8437 - accuracy: 0.1917 - val_loss: 1.7870 - val_accuracy: 0.1367\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 21s 711ms/step - loss: 1.7406 - accuracy: 0.2113 - val_loss: 1.7758 - val_accuracy: 0.1641\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 22s 722ms/step - loss: 1.6481 - accuracy: 0.2278 - val_loss: 1.7755 - val_accuracy: 0.1680\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 22s 728ms/step - loss: 1.5605 - accuracy: 0.2451 - val_loss: 1.8102 - val_accuracy: 0.1523\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 22s 730ms/step - loss: 1.4735 - accuracy: 0.2669 - val_loss: 1.8187 - val_accuracy: 0.1719\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 22s 738ms/step - loss: 1.3952 - accuracy: 0.2903 - val_loss: 1.8526 - val_accuracy: 0.1484\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 22s 736ms/step - loss: 1.3174 - accuracy: 0.3174 - val_loss: 1.8798 - val_accuracy: 0.1719\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 22s 729ms/step - loss: 1.2376 - accuracy: 0.3484 - val_loss: 1.8940 - val_accuracy: 0.1797\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 22s 734ms/step - loss: 1.1701 - accuracy: 0.3777 - val_loss: 1.9319 - val_accuracy: 0.1523\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 21s 710ms/step - loss: 1.1063 - accuracy: 0.4020 - val_loss: 1.9392 - val_accuracy: 0.1484\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 21s 715ms/step - loss: 1.0354 - accuracy: 0.4335 - val_loss: 1.9862 - val_accuracy: 0.1523\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 22s 717ms/step - loss: 0.9709 - accuracy: 0.4625 - val_loss: 1.9887 - val_accuracy: 0.1562\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 22s 720ms/step - loss: 0.9147 - accuracy: 0.4879 - val_loss: 2.0337 - val_accuracy: 0.1328\n",
            "Epoch 22/50\n",
            "30/30 [==============================] - 22s 722ms/step - loss: 0.8592 - accuracy: 0.5174 - val_loss: 2.0384 - val_accuracy: 0.1523\n",
            "Epoch 23/50\n",
            "30/30 [==============================] - 22s 716ms/step - loss: 0.8163 - accuracy: 0.5330 - val_loss: 2.0430 - val_accuracy: 0.1523\n",
            "Epoch 24/50\n",
            "30/30 [==============================] - 22s 717ms/step - loss: 0.7627 - accuracy: 0.5655 - val_loss: 2.0898 - val_accuracy: 0.1641\n",
            "Epoch 25/50\n",
            "30/30 [==============================] - 22s 720ms/step - loss: 0.7271 - accuracy: 0.5785 - val_loss: 2.0844 - val_accuracy: 0.1641\n",
            "Epoch 26/50\n",
            "30/30 [==============================] - 22s 717ms/step - loss: 0.6796 - accuracy: 0.6075 - val_loss: 2.1020 - val_accuracy: 0.1602\n",
            "Epoch 27/50\n",
            "30/30 [==============================] - 21s 715ms/step - loss: 0.6365 - accuracy: 0.6316 - val_loss: 2.1216 - val_accuracy: 0.1602\n",
            "Epoch 28/50\n",
            "30/30 [==============================] - 21s 716ms/step - loss: 0.6012 - accuracy: 0.6524 - val_loss: 2.1113 - val_accuracy: 0.1602\n",
            "Epoch 29/50\n",
            "30/30 [==============================] - 22s 723ms/step - loss: 0.5703 - accuracy: 0.6718 - val_loss: 2.1672 - val_accuracy: 0.1602\n",
            "Epoch 30/50\n",
            "30/30 [==============================] - 22s 718ms/step - loss: 0.5357 - accuracy: 0.6918 - val_loss: 2.1723 - val_accuracy: 0.1680\n",
            "Epoch 31/50\n",
            "30/30 [==============================] - 22s 730ms/step - loss: 0.5022 - accuracy: 0.7120 - val_loss: 2.2131 - val_accuracy: 0.1641\n",
            "Epoch 32/50\n",
            "30/30 [==============================] - 21s 711ms/step - loss: 0.4687 - accuracy: 0.7340 - val_loss: 2.2248 - val_accuracy: 0.1484\n",
            "Epoch 33/50\n",
            "30/30 [==============================] - 22s 723ms/step - loss: 0.4412 - accuracy: 0.7526 - val_loss: 2.2383 - val_accuracy: 0.1680\n",
            "Epoch 34/50\n",
            "30/30 [==============================] - 22s 728ms/step - loss: 0.4161 - accuracy: 0.7687 - val_loss: 2.2377 - val_accuracy: 0.1641\n",
            "Epoch 35/50\n",
            "30/30 [==============================] - 22s 734ms/step - loss: 0.3876 - accuracy: 0.7856 - val_loss: 2.2772 - val_accuracy: 0.1719\n",
            "Epoch 36/50\n",
            "30/30 [==============================] - 22s 736ms/step - loss: 0.3638 - accuracy: 0.8028 - val_loss: 2.3134 - val_accuracy: 0.1523\n",
            "Epoch 37/50\n",
            "30/30 [==============================] - 22s 723ms/step - loss: 0.3417 - accuracy: 0.8157 - val_loss: 2.3112 - val_accuracy: 0.1641\n",
            "Epoch 38/50\n",
            "30/30 [==============================] - 22s 740ms/step - loss: 0.3161 - accuracy: 0.8348 - val_loss: 2.3311 - val_accuracy: 0.1562\n",
            "Epoch 39/50\n",
            "30/30 [==============================] - 22s 738ms/step - loss: 0.2980 - accuracy: 0.8439 - val_loss: 2.3713 - val_accuracy: 0.1562\n",
            "Epoch 40/50\n",
            "30/30 [==============================] - 22s 720ms/step - loss: 0.2777 - accuracy: 0.8586 - val_loss: 2.3495 - val_accuracy: 0.1641\n",
            "Epoch 41/50\n",
            "30/30 [==============================] - 22s 725ms/step - loss: 0.2594 - accuracy: 0.8700 - val_loss: 2.3987 - val_accuracy: 0.1562\n",
            "Epoch 42/50\n",
            "30/30 [==============================] - 21s 712ms/step - loss: 0.2425 - accuracy: 0.8802 - val_loss: 2.4400 - val_accuracy: 0.1562\n",
            "Epoch 43/50\n",
            "30/30 [==============================] - 21s 707ms/step - loss: 0.2249 - accuracy: 0.8911 - val_loss: 2.4175 - val_accuracy: 0.1602\n",
            "Epoch 44/50\n",
            "30/30 [==============================] - 21s 715ms/step - loss: 0.2102 - accuracy: 0.9026 - val_loss: 2.4443 - val_accuracy: 0.1797\n",
            "Epoch 45/50\n",
            "30/30 [==============================] - 22s 731ms/step - loss: 0.1997 - accuracy: 0.9077 - val_loss: 2.4509 - val_accuracy: 0.1523\n",
            "Epoch 46/50\n",
            "30/30 [==============================] - 21s 704ms/step - loss: 0.1877 - accuracy: 0.9168 - val_loss: 2.4904 - val_accuracy: 0.1562\n",
            "Epoch 47/50\n",
            "30/30 [==============================] - 21s 715ms/step - loss: 0.1738 - accuracy: 0.9235 - val_loss: 2.4906 - val_accuracy: 0.1562\n",
            "Epoch 48/50\n",
            "30/30 [==============================] - 21s 713ms/step - loss: 0.1650 - accuracy: 0.9302 - val_loss: 2.5291 - val_accuracy: 0.1562\n",
            "Epoch 49/50\n",
            "30/30 [==============================] - 22s 718ms/step - loss: 0.1514 - accuracy: 0.9395 - val_loss: 2.5417 - val_accuracy: 0.1602\n",
            "Epoch 50/50\n",
            "30/30 [==============================] - 21s 707ms/step - loss: 0.1414 - accuracy: 0.9446 - val_loss: 2.5642 - val_accuracy: 0.1445\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fadfeec88d0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya4OGUvumHG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmjF11J0zJQn"
      },
      "source": [
        "# rlp = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "#     monitor='val_loss', factor=0.1, patience=5, verbose=10,\n",
        "#     mode='auto', min_delta=0.00001, cooldown=0, min_lr=0.0001,\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DpC9zlzMcXp"
      },
      "source": [
        "## <font color='blue'>**Inference**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5NhESYyMW_t"
      },
      "source": [
        "<font color='blue'>**Plot attention weights**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkEY7SsBMtrC"
      },
      "source": [
        "# https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "  sequences = [[list(), 0.0]]\n",
        "  # print(\"asd\")\n",
        "  for row in data:\n",
        "    all_candidates = list()\n",
        "    for i in range(len(sequences)):\n",
        "      seq, score = sequences[i]\n",
        "      for j in range(len(row)):\n",
        "        try:\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "        except ValueError as e:\n",
        "          candidate = [seq + [j], 0]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    # select k best\n",
        "    sequences = ordered[:k]\n",
        "  return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9iUbnHSLRY"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "def evaluate(sentence):\n",
        "  max_length_targ = 40\n",
        "  max_length_inp  = 39\n",
        "  \n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "  # sentence = preprocess(sentence)\n",
        "  # sentence = sentence.strip()\n",
        "  # print(sentence)\n",
        "  #pdb.set_trace()\n",
        "  inputs = [] \n",
        "  # for word in sentence.split():\n",
        "  #     print(word)\n",
        "  #     inputs.append(tokenizer.word_index[word])\n",
        "  inputs = tokenizer.texts_to_sequences([sentence])\n",
        "  inputs =  tf.keras.preprocessing.sequence.pad_sequences(inputs,maxlen=max_length_inp,padding='post') \n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = '' \n",
        "\n",
        "  initial_state=model2.layers[0].initialize_states(batch_size=1)\n",
        "  encoder_outputs, state_h,state_c = model2.layers[0](inputs,initial_state)   \n",
        "  dec_input = tf.expand_dims([tokenizer_e.word_index['<start>']], 0)\n",
        "  # dec_input=tf.expand_dims([tokenizer_e.word_index['\\t']],0)\n",
        "\n",
        "\n",
        "  for i in range(max_length_targ):\n",
        "    Output,state_h,state_c,att_weights,_ = model2.layers[1].oneStepDecoder(dec_input,encoder_outputs,state_h,state_c)\n",
        "    #Beam Search Decoder\n",
        "    Result_beam_list=beam_search_decoder(Output,k=1)\n",
        "    Result_beam=Result_beam_list[0][0]\n",
        "\n",
        "    attention_weights = tf.reshape(att_weights, (-1, ))\n",
        "    attention_plot[i]  = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(Output[0]).numpy()\n",
        "  \n",
        "    result += tokenizer_e.index_word[Result_beam[0]] + ' '\n",
        "    if tokenizer_e.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR87eW8lOzt9",
        "outputId": "3721d822-b9e8-46de-b897-8939c53cd050"
      },
      "source": [
        "tokenizer_e.word_index[\"<end>\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3899"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6D07RIJNDvy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)  \n",
        "    ax.xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-HIpuKDM88T"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    print(\"-\"*50)\n",
        "    # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    # plot_attention(attention_plot, sentence.split(' '),    result.split(' '))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxmLpST4NOd5",
        "outputId": "b08cc3ee-93c5-440f-fce9-cae62ba9c43d"
      },
      "source": [
        "result=translate(test_data[\"SMS_TEXT\"][889])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: fun..How i wish i can spend this coming v day with u..Hee..Can?Future guy\n",
            "Predicted translation: I am 25 male. Chinese Malaysian. <end> \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYcUAVYae4Oh",
        "outputId": "05f5b370-a61e-4340-c0a2-6c8873b376d6"
      },
      "source": [
        "result=translate(test_data[\"SMS_TEXT\"][488])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I will come at 8.40pm\n",
            "Predicted translation: I will be working until also will be ok with me and CC my place and that will you be 2:45. Then I will wait for me also. <end> \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95ZaIAtGb3k_",
        "outputId": "e8b59c30-8e97-46cf-ed8c-24785cea81a5"
      },
      "source": [
        "tokenizer.word_index[\"you.\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "685"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4HDPy50OFJC",
        "outputId": "da8aebb3-9286-4a20-bafb-b8f66cbc66fb"
      },
      "source": [
        "test_data[\"SMS_TEXT\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "889     fun..How i wish i can spend this coming v day ...\n",
              "1675       Are you going to send a mail... Tmr i cant leh\n",
              "416     i cant understand my linear algebra tutor. i t...\n",
              "1083    Yupz... Muz stick e foto on e booklet ma... Ok...\n",
              "1105                Lazing around at home lor... u lei...\n",
              "1000                              Can just go down right?\n",
              "1219    Hey ü wanna meet outside e lt? Haha... Time's ...\n",
              "1440    ANGEL y no reply leh?if u wan u can msg me at ...\n",
              "769     Me n my sis eatin bfast... Hee... U call me lo...\n",
              "1809    Exercise til wat time? Aft exercise la...Fat fat.\n",
              "452         Guess wat im e one whose gonna b late..gee...\n",
              "488                                 I will come at 8.40pm\n",
              "1986    My sis so bo liao u noe. She arguing w me abt ...\n",
              "1308          545 can... Cos i finish work at tis time...\n",
              "1829    I already outside pl come early. reach here, c...\n",
              "1184    ya.. found him already la.. of cos.. wat time ...\n",
              "124     Just came to nydc n she just ordered a baked r...\n",
              "819     Nope... I'm reaching home. Take my bag then go...\n",
              "1585    Dunno but no choice he very dirty... Then how ...\n",
              "1672    Hi Kote & Maxy! Pls. intro & tel no. My no 016...\n",
              "Name: SMS_TEXT, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeLarAhIT9VK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXVoNapqXDEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d9b927-41d1-4c74-8ec4-d4102c7eef2c"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from tqdm import tqdm\n",
        "def BleuScore(validation):\n",
        "    input=list(validation['SMS_TEXT'])\n",
        "    Y_true=list(validation['ENGLISH_OUTPUT'])\n",
        "    results = []\n",
        "    bleuscores=[]\n",
        "    input_sent_list = []\n",
        "    # print(len(input))\n",
        "    for i in tqdm(range(len(input))):\n",
        "      try:\n",
        "        # print(\"1\")\n",
        "        result, sentence, attention_plot = evaluate(input[i])\n",
        "        # print(result)\n",
        "        \n",
        "        results.append(result)  \n",
        "        input_sent_list.append(sentence)    \n",
        "        bleuscores.append(bleu.sentence_bleu(Y_true[i], result))\n",
        "        # print(\":sss\")\n",
        "      except KeyError as e:\n",
        "        pass  \n",
        "    return sum(bleuscores)/len(bleuscores),bleuscores,results,input_sent_list\n",
        "\n",
        "AvearageScore,bleuscores,results,input_sent_list=BleuScore(test_data)\n",
        "\n",
        "print(\"Average Bleuscore:\",AvearageScore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [14:27<00:00, 43.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Bleuscore: 0.7247005670864131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1IhdBrgQYJr"
      },
      "source": [
        "<font color='blue'>**Predict the sentence translation**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDT91Im6MZS6"
      },
      "source": [
        "\n",
        "def predict(input_sentence):\n",
        "\n",
        "  '''\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "  C. Initialize index of <\\t> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
        "  D. till we reach max_length of decoder or till the model predicted character <\\n>:\n",
        "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
        "         And get the character using the tokenizer(character index) and then store it in a string.\n",
        "  E. Return the predicted sentence\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  input_sequence=tokenizer.texts_to_sequences([input_sentence])\n",
        "  \n",
        "\n",
        "  inputs=pad_sequences(input_sequence,maxlen=39,padding='post')\n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "  result=''\n",
        "  units=100\n",
        "  hidden=[tf.zeros((1,units))]\n",
        "  encoder_output,hidden_state,cell_state=model2.layers[0](inputs,hidden)\n",
        "  dec_hidden=hidden_state\n",
        "  dec_input=tf.expand_dims([tokenizer_e.word_index['<start>']],0)\n",
        "  for t in range(40):\n",
        "      predictions,dec_hidden,cell_state,attention_weights,context_vector=model2.layers[1].oneStepDecoder(dec_input,encoder_output,dec_hidden,cell_state)\n",
        "\n",
        "      predicted_id=tf.argmax(predictions[0]).numpy()\n",
        "      result+=tokenizer_e.index_word[predicted_id] + ' '\n",
        "      if tokenizer_e.word_index['<end>']==predicted_id:\n",
        "          return result\n",
        "      dec_input= tf.expand_dims([predicted_id],0)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z-As25YMd47",
        "outputId": "c7587e3c-4ea7-45e4-d3f9-0ee4c18e1987"
      },
      "source": [
        "for index in range(len(test_data)):\n",
        "    print('*'*200)\n",
        "    print(\"SMS_TEXT\")\n",
        "    print(test_data['SMS_TEXT'].values[index])\n",
        "    print(\"Prediction\")\n",
        "    print(predict(test_data['SMS_TEXT'].values[index]))\n",
        "    print(\"Original Sentence\")\n",
        "    print(test_data['ENGLISH_OUTPUT'].iloc[index])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "fun..How i wish i can spend this coming v day with u..Hee..Can?Future guy\n",
            "Prediction\n",
            "I am 25 male. Chinese Malaysian. <end> \n",
            "Original Sentence\n",
            "Fun. How I wish I can spend this coming valentine's day with you. Hee. Can? Future guy. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Are you going to send a mail... Tmr i cant leh\n",
            "Prediction\n",
            "Are you free to the canteen for dessert. Do you want to meet up. <end> \n",
            "Original Sentence\n",
            "Are you going to send a mail? Tomorrow I can't. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "i cant understand my linear algebra tutor. i think hes from china. cannot understand what hes saying.\n",
            "Prediction\n",
            "I cannot go home before your dog. I feel not work out for a break already. <end> \n",
            "Original Sentence\n",
            "I can't understand my Linear Algebra tutor. I think he is from China. Cannot understand what he is saying. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Yupz... Muz stick e foto on e booklet ma... Okie, meet at 1230?\n",
            "Prediction\n",
            "Just bring. In only coming home? <end> \n",
            "Original Sentence\n",
            "Yes, must stick the photo on the booklet. Okay, meet at 12:30? <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Lazing around at home lor... u lei...\n",
            "Prediction\n",
            "Oh. Tomorrow you are at home around 2 club as possible. <end> \n",
            "Original Sentence\n",
            "Lazing around at home. You? <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Can just go down right?\n",
            "Prediction\n",
            "Can I do dance but she like that? The aluminium one more? Thank you! <end> \n",
            "Original Sentence\n",
            "Can just go down, right? <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Hey ü wanna meet outside e lt? Haha... Time's a little tight though...\n",
            "Prediction\n",
            "Hey, you are going to get the lecture tomorrow. <end> \n",
            "Original Sentence\n",
            "Hey, do you want to meet outside the lecture theatre? Haha. Time's a little tight though. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "ANGEL y no reply leh?if u wan u can msg me at 96473920.\n",
            "Prediction\n",
            "Angel, can I ask your IC number and address? You have the number. <end> \n",
            "Original Sentence\n",
            "Angel why you did not reply? If you want you can message me at 96473920. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Me n my sis eatin bfast... Hee... U call me lor...\n",
            "Prediction\n",
            "I don't understand her at what night I just have free so I have no need go at 12? I'm going to watch. <end> \n",
            "Original Sentence\n",
            "My sister and I are eating breakfast. You call me. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Exercise til wat time? Aft exercise la...Fat fat.\n",
            "Prediction\n",
            "Then Millian's membership card number, how are you? Care to you, then are you from? <end> \n",
            "Original Sentence\n",
            "Exercise till what time? After exercise. Fat. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Guess wat im e one whose gonna b late..gee...\n",
            "Prediction\n",
            "Hey, sorry you are going to the Mango shop tomorrow. See you know what to do you want to meet up. Haha. <end> \n",
            "Original Sentence\n",
            "Guess what? I'm the one who is going to be late. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "I will come at 8.40pm\n",
            "Prediction\n",
            "I will be working until also will be ok with me and CC my place and that will you be 2:45. Then I will wait for me also. <end> \n",
            "Original Sentence\n",
            "I will come at 8:40 pm . <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "My sis so bo liao u noe. She arguing w me abt sun's hair den accuse me of smethg i haven thgt of. Hate it when pple accuse me...\n",
            "Prediction\n",
            "My sister has got vet? Will be too flowery in that time. <end> \n",
            "Original Sentence\n",
            "My sister does silly things you know. She was arguing with me about Sun's hair and then she accuse me of something I haven't thought of. I hate it when people accuse me. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "545 can... Cos i finish work at tis time...\n",
            "Prediction\n",
            "Hello everyone! Nice to go for lesson on Monday. So she didn't do some weeks'tutorials first. Then we are meeting for dinner? <end> \n",
            "Original Sentence\n",
            "At 5:45 I can. Because I finish work at this time. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "I already outside pl come early. reach here, call me.\n",
            "Prediction\n",
            "I was on Saturday, I will be at the lecture then. <end> \n",
            "Original Sentence\n",
            "I am already outside, please come early. Reach here, call me. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "ya.. found him already la.. of cos.. wat time ur proj until? u eat my pasta already? nice?\n",
            "Prediction\n",
            "No. Maybe not meeting. Other days then don't think so? <end> \n",
            "Original Sentence\n",
            "Yes. Found him already. Of course. What time is your project until? You eat my pasta already? Nice? <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Just came to nydc n she just ordered a baked rice n i ordered a drink. U done liao.\n",
            "Prediction\n",
            "Just help you a testimonial already. Haha. <end> \n",
            "Original Sentence\n",
            "Just came to nydc, she just ordered a baked rice and I ordered a drink. You done already. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Nope... I'm reaching home. Take my bag then go sch.\n",
            "Prediction\n",
            "No. I'm stuck already in computer configured. I pick me some work to buy? <end> \n",
            "Original Sentence\n",
            "No. I'm reaching home. Take my bag and then go to school. <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Dunno but no choice he very dirty... Then how change back...\n",
            "Prediction\n",
            "I don't know if they can go and cut. Can I cannot go to fish and wait for my first paper. <end> \n",
            "Original Sentence\n",
            "Don't know but no choice he is very dirty. Then how to change back? <end>\n",
            "********************************************************************************************************************************************************************************************************\n",
            "SMS_TEXT\n",
            "Hi Kote & Maxy! Pls. intro & tel no. My no 0166305681\n",
            "Prediction\n",
            "Hi Angel, did you know today? <end> \n",
            "Original Sentence\n",
            "Hi Kote and Maxy! Please give me your introduction and telephone number. My number is 0166305681. <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BdPFxK6Pasd",
        "outputId": "2fdbd46a-3680-48b4-ed0b-cf173f9cb8d5"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "sum=0\n",
        "for index in range(len(test_data)):\n",
        "    reference=test_data['ENGLISH_OUTPUT'].iloc[index]\n",
        "    translation=predict(test_data['SMS_TEXT'].iloc[index])\n",
        "    sum+=bleu.sentence_bleu(reference,translation)\n",
        "print(sum/len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6780770304835106\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}